\section{División}
El primer paso para analizar identificadores consiste en dividir, cada uno de ellos, en los elementos que lo conforman.
Los desarrolladores componen estos identificadores concatenando palabras y abreviaturas, a veces, empleando convenciones claras en la demarcación de sus partes, como lo son el uso de caracteres no alfabéticos (por ejemplo: ``\_'' o ``-''), o la técnica camel-case, donde la primera letra de cada palabra se escribe en mayúsculas a excepción de la inicial.
Esto permite que la división automática se lleve a cabo sin ningún problema.
Sin embargo, en los casos en los que no se implementa una clara delimitación es donde surge la necesidad de aplicar técnicas más sofisticadas para lograr la separación de las partes.
Aquí es donde entran en juego los \textbf{algoritmos de división}.
 
El objetivo de un algoritmo divisor de identificadores es tomar uno de estos últimos como entrada, y obtener como salida una lista de sub-elementos que particionan al identificador original.
Estos sub-elementos pueden ser palabras de diccionario, las cuales tienen un significado obvio; abreviaturas, las cuales refieren a una sola palabra del diccionario; o acrónimos, los cuales representan varias palabras de diccionario \cite{HillBinkleyLawrie14}.

\subsection{El problema de la división de tokens}
Ahora bien, estos algoritmos deben ser capaces de afrontar una serie de problemas relacionados a la correcta división de los identificadores.
Como se dijo anteriormente, una forma de establecer la separación entre las palabras componentes de un indicador es el uso de caracteres especiales, o a través de la técnica de \textit{camel-casing}.
Esta última permite reducidir la cantidad de caracteres sin comprometer la legibilidad, aunque existen situaciones donde no hay convenciones establecidas y esto puede afectar negativamente la comprensión, siendo ejemplos de este caso la incorporación de acrónimos al identificador (ej: \textit{sqlList, SQLList, SQLlist}) o la no utilización de delimitadores para conceptos multi palabras que son de uso común, como en \textit{sizeof} o \textit{hostname}.

Un token puede ser definido formalmente como $t = (s_0, s_1, s_2, ..., s_n)$ donde $s_i$ es una letra, un dígito o un caracter especial.
El primer paso consiste en separar el token utilizando los dígitos y caracteres especiales como marcadores.
Cada sub-string es considerado como un token potencialmente divisible y recibe el nombre de \textit{token alfabético}.
Existen cuatro casos posibles a la hora de decidir si dividir el token en un cierto punto entre $s_i$ y $s_j$\cite{EnslenHillPollock09}:

\begin{enumerate}
  \item $s_i$ está en minúsculas y $s_j$ en mayúsculas (ej: getString, setPoint)
  \item $s_i$ está en mayúsculas y $s_j$ en minúsculas (ej: getMAXstring, GPSstate, ASTVisitor)
  \item tanto $s_i$ como $s_j$ están en minúsculas (ej: notype, databasefield, actionparameters)
  \item tanto $s_i$ como $s_j$ están en mayúsculas (ej: USERLIB, NONNEGATIVEDECIMALTYPE, COUNTRYCODE)
\end{enumerate}

El primer caso, es el lugar natural para realizar la división.
En el segundo caso, existen mayúsculas y minúsculas alternadas en el probable punto de corte, lo que que requiere enfrentar el problema de la separación de tokens \textit{mixed-case}, particularmente complicado por la utilización de acrónimos.
Por último, los casos 3 y 4 corresponden al problema de la separación de tokens \textit{same-case}; en el cual los programadores no han dejado rastro en dónde deben ser aplicados los puntos de corte.

Un algoritmo de división de tokens completamente automático debería ser capaz de resolver los sub-problemas de mixed-case y same-case, de forma efectiva.

\subsection{Estado del arte}
De acuerdo a la recopilación de Hill et al \cite{HillBinkleyLawrie14}, algunas de las principales técnicas para la división de identificadores son:

\begin{itemize}
  \item \textbf{Greedy.} Esta técnica utiliza un diccionario, una lista de abreviaturas conocidas y una lista de corte, la cual incluye identificadores predefinidos, librerías, funciones, nombres de variables comunes y letras individuales.
Después de retornar cada hard word encontrada en alguna de las tres listas, como una soft word simple, el resto de las hard words se consideran para división.
A partir de ahí, recursivamente, se analizan los sufijos y prefijos de las palabras hasta que se encuentren en alguna de las listas, prefiriendo siempre las palabras de mayor longitud.
  
  \item \textbf{Samurai.} Este algoritmo se basa en la premisa de que las partes que componen a los identificadores multi-palabras de un determinado programa, suelen ser utilizados en algún otro lugar, así sea en el mismo software como en otros.
Por lo tanto, la frecuencia de aparición de las palabras es el principal elemento considerado a la hora de realizar las divisiones.
  
  \item \textbf{GenTest.} Tiene su fuerte en la división de términos same-case, generando primero todas las posibles separaciones (dado que los identificadores son relativamente cortos, el potencialmente exponencial número de fragmentos es manejable en la práctica), para luego aplicar un \textit{scoring} sobre cada elemento, seleccionando la de mayor valor.
Esta función de scoring se apoya en que las soft words iguales o similares, deberían encontrarse ubicadas cerca una de otra en la documentación o texto general (métrica de similaridad). 
  
  \item \textbf{DTW.} Esta técnica se basa en la observación de que los programadores construyen nuevos identificadores aplicando un conjunto de transformaciones a las palabras, como por ejemplo quitar todas las vocales o algunos caracteres.
Utilizando un diccionario que contenga palabras y términos pertenecientes a una ontología superior, al dominio de la aplicación o ambos, el objetivo consiste en identificar un nivel de coincidencia cuasi-óptimo entre las partes del identificador y las palabras en el diccionario.
  
  \item \textbf{INTT.} El enfoque INTT busca realizar una división más precisa que las técnicas previas al utilizar una heurística especializada para manejar identificadores con dígitos, sin removerlos del texto en etapas tempranas del proceso de división.
Las principales modificaciones consisten en reemplazar greedy por dos algoritmos, greedy y greedier, los cuales pueden reconocer las partes de los identificadores same-case, sin requerir que comiencen o terminen con una palabra conocida; y además utilizar una lista de acrónimos que contiene dígitos.
\end{itemize}

Para el presente informe, sólo son de interés - y por lo tanto explicadas en mayor detalle - las primeras tres técnicas listadas anteriormente.

\subsection{Algoritmo Greedy}
Desarrollado por Feild, Binkley and Lawrie \cite{FieldBinkleyLawrie06, Feild06anempirical, Lawrie2007, Lawrie:2007:EMA:1306878.1307350, EnslenHillPollock09}, este algoritmo recibe su nombre gracias al enfoque que toma para resolver el problema, eligiendo en cada paso local la opción más conveniente con la idea de lograr, a nivel general, una solución óptima.
El proceso comienza realizando una búsqueda por cada hard word dentro de un conjunto de listas de palabras y abreviaturas.
Si la palabra candidata existe en alguna de ellas, se devuelve como válida, si no, se asume que se conforma por múltiples palabras, y éstas son tratadas recursivamente hasta encontrar sus componentes \cite{Feild06anempirical}.
Las tres listas que se utilizan son:

\begin{itemize}
  \item \textbf{Diccionario:} se conforma con diccionarios de dominio público, los cuales son consultados en la herramienta de verificación de gramática \textit{ispell}, disponible en Linux.
  
  \item \textbf{Abreviaturas conocidas:} consiste de abreviaturas que son de uso común y relacionadas tanto al dominio del problema como a la programación.
  
  \item \textbf{Stop-list:} es la lista de corte y está compuesta de tres tipos de palabras, dentro de las cuales encontramos a los tipos de datos predefinidos, variables globales (tanto del lenguage como del ambiente), y los nombres de librerías estándar.
\end{itemize}

Tal como puede verse en el algoritmo \ref{algGreedy}, el proceso comienza dividiendo el token de entrada en hard-words, tomando como delimitadores a los caracteres especiales y la técnica de \textit{camel-casing}.
Una vez hechas las divisiones tanto por números, caracteres especiales (línea 4), como por el cambio de minúsculas a mayúsculas en CamelCase (línea 5), se procede con la evaluación de cada una de las hard-words resultantes.
En primera instancia, se verifica si la palabra candidata existe en alguna de las listas con las que trabaja el algoritmo (línea 8).
Si es así, el fragmento del token es considerado parte de la división y por lo tanto de la salida del proceso (línea 13).
En caso contrario, al asumirse que está formado por múltiples palabras, se procesa en búsqueda de las softwords que lo componen.
Este análisis consiste en realizar dos búsquedas sobre cada hardword, quedándose con el mejor resultado entre ambas.
En la primera búsqueda (línea 9) se trata de obtener el prefijo más largo que se encuentre en una de las listas, y forme parte de la palabra actual.
Luego se llama recursivamente, trabajando sobre el fragmento restante de la palabra original (prefijos y sufijos).
La segunda búsqueda (línea 10) es idéntica, excepto que trata de obtener el sufijo más largo.
Los resultados de ambos procesamientos son evaluados, y aquel que arroje la relación más alta entre la cantidad de softwords encontradas en listas en comparación con el total de palabras, es el elegido (línea 11).
Al finalizar la ejecución del algoritmo, la salida que se obtiene es el identificador original dividido, separando cada una de los componentes encontrados, a través de espacios en blanco (línea 16).

\begin{algorithm}[H]
\caption{Greedy}
\label{algGreedy}
\DontPrintSemicolon
  \SetKwData{Token}{token}
  \SetKwData{SplitToken}{splitToken}
  \SetKwData{Dictionary}{dictionary}
  \SetKwData{KnownAbbrs}{knownAbbreviations}
  \SetKwData{StopList}{stopList}
  \SetKwData{Preffix}{preffix}
  \SetKwData{Suffix}{suffix}
  
  \SetKwFunction{SplitChars}{splitOnSpecialCharactersAndDigits(\Token)}
  \SetKwFunction{SplitLowerUpperCase}{splitOnLowerCaseToUpperCase(\Token)}
  \SetKwFunction{FindPreffix}{findPreffix($s_i$,$""$)}
  \SetKwFunction{FindSuffix}{findSuffix($s_i$,$""$)}
  \SetKwFunction{Compare}{compare(\Preffix,\Suffix)}
  
  \KwIn{\Token, para ser divido}
  \KwOut{\SplitToken, token dividido delimitado por espacios}
  
  \BlankLine
  var \Dictionary\;
  var \KnownAbbrs\;
  var \StopList\;
  
  \BlankLine
  \Token $\leftarrow$ \SplitChars\;
  \Token $\leftarrow$ \SplitLowerUpperCase\;
  \SplitToken $\leftarrow$ $""$\;
  
  \BlankLine
  \ForEach{$s_i \in \Token$}{
    \eIf{$s_i \not \in (\Dictionary \cup \KnownAbbrs \cup \StopList)$}{
      \Preffix $\leftarrow$ \FindPreffix\;
      \Suffix $\leftarrow$ \FindSuffix\;
      \SplitToken $\leftarrow$ \Compare\;
    }
    {\SplitToken $\leftarrow$ $s_i$}
  }
  \BlankLine
  \KwRet \SplitToken\;
\end{algorithm}

La función \textit{findPreffix} es descrita en el algoritmo \ref{funFindPreffix}. 
La primera validación que se realiza sirve como punto de corte para la recursión (línea 1), cuando el token a analizar está vacío.
Si el parámetro recibido es parte de alguna de las listas (línea 4), la función finaliza separando el prefijo encontrado y llamándose recursivamente con el fragmento restante (línea 5).
En caso contrario, el último caracter es removido (línea 8) y el proceso continúa llamándose a si mismo (línea 9).
Los caracteres removidos son agrupados (línea 7) y luego agregados al resultado. 
Si la primera palabra en el identificador dividido no está en una de las listas, los caracteres se van acoplando. Si lo está, se agregan como una nueva softword.

\begin{algorithm}
\caption{Función findPreffix}
\label{funFindPreffix}
\DontPrintSemicolon
  \SetKwData{S}{s}
  \SetKwData{SSplit}{sSplit}
  \SetKwData{Dictionary}{dictionary}
  \SetKwData{KnownAbbrs}{knownAbbreviations}
  \SetKwData{StopList}{stopList}
  
  \SetKwFunction{Size}{size}
  \SetKwFunction{FindPreffixSSplitEmpty}{findPreffix(\SSplit,$""$)}
  \SetKwFunction{FindPreffixSSSplit}{findPreffix(\S,\SSplit)}
  
  \KwIn{\S, string para ser analizado}
  \KwOut{\SSplit, string dividido y delimitado por espacios}
  
  \BlankLine
  \If{$\S.\Size == 0$}{
    \KwRet \SSplit
  }
  
  \BlankLine
  \If{$\S \in (\Dictionary \cup \KnownAbbrs \cup \StopList)$}{
    \KwRet $\S + "" + \FindPreffixSSplitEmpty$ 
  }
  
  \BlankLine
  \SSplit $\leftarrow \S[length(\S) - 1] + \SSplit$\;
  
  \BlankLine
  \S $\leftarrow \S[0, length(\S) - 1]$\;
  
  \BlankLine
  \KwRet \FindPreffixSSSplit
\end{algorithm}

Una lógica similar se sigue para la función \textit{findSuffix}, visible en el algoritmo \ref{funFindSuffix}. 
Cuando el parámetro se encuentra de alguna de las listas (línea 4), el sufijo se extrae y la función continúa llamándose recursivamente (línea 5).
En caso contrario, el primer caracter es removido (línea 8) y el proceso continúa llamándose a si mismo (línea 9), reduciendo la longitud del sufijo a evaluar.
Al igual que en la otra función, los caracteres removidos son agrupados (línea 7) y luego agregados al resultado.

\begin{algorithm}
\caption{Función findSuffix}
\label{funFindSuffix}
\DontPrintSemicolon
  \SetKwData{S}{s}
  \SetKwData{SSplit}{sSplit}
  \SetKwData{Dictionary}{dictionary}
  \SetKwData{KnownAbbrs}{knownAbbreviations}
  \SetKwData{StopList}{stopList}
  
  \SetKwFunction{Size}{size}
  \SetKwFunction{FindSuffixSSplitEmpty}{findSuffix(\SSplit,$""$)}
  \SetKwFunction{FindSuffixSSSplit}{findSuffix(\S,\SSplit)}
  
  \KwIn{\S, string para ser analizado}
  \KwOut{\SSplit, string dividido y delimitado por espacios}
  
  \BlankLine
  \If{$\S.\Size == 0$}{
    \KwRet \SSplit
  }
  
  \BlankLine
  \If{$\S \in (\Dictionary \cup \KnownAbbrs \cup \StopList)$}{
    \KwRet $\FindSuffixSSplitEmpty + "" + \S$ 
  }
  
  \BlankLine
  \SSplit $\leftarrow \SSplit + \S[0] $\;
  
  \BlankLine
  \S $\leftarrow \S[1, length(\S)]$\;
  
  \BlankLine
  \KwRet \FindSuffixSSSplit
\end{algorithm}

Este algoritmo, en comparación con otros existentes, tiene la ventaja de que es independiente del conjunto de datos sobre el que se ejecuta.
Además, su lógica e implementación, son sencillas, lo que lo convierte en el punto de comparación a la hora de evaluar el rendimiento de otras propuestas.

Sin embargo, tiene algunos aspectos negativos.
El tiempo de ejecución puede ser elevado, ya que la inicialización está dominada por la carga de varios diccionarios en tablas \textit{hash} y por la utilización de recursividad \cite{FieldBinkleyLawrie06}.
Otra desventaja que presenta es que tiende a realizar una mayor cantidad de divisiones que las esperadas \cite{Feild06anempirical}.

\subsection{Algoritmo Samurai}
Propuesto por Hill et al., este algoritmo está inspirado en un trabajo previo, sobre el minado automático de código fuente para la expansión de abreviaturas \cite{Hill:2008:AAM:1370750.1370771}.
Las hipótesis sobre las que basan la lógica del proceso son dos. 
En primera instancia, las cadenas de texto que componen los tokens multi-palabras tienden a ser empleadas en alguna otra parte del código, tanto del mismo programa como de otros.
Estos tokens pueden aparecer tanto como una palabra independiente, o como parte de una composición.
En segunda instancia, se apoyan en la hipótesis de que las divisiones suelen ser en favor de aquellos tokens que ocurren más a menudo en un programa.
Por lo tanto, la frecuencia de una palabra se utiliza para determinar los cortes en el token analizado.

Para poder llevar adelante la división, es necesario explorar el conjunto de tokens existentes en el código y crear dos tablas de frecuencia.
Este proceso de minado se realiza ejecutando el algoritmo de división de tokens conservativo basado en marcadores de separación (caracteres especiales) sobre un conjunto de tokens del código fuente, para así generar un listado de todas las ocurrencias de los strings delimitados por marcadores.
Este listado es luego procesado para obtener la información relativa de frecuencias, en la forma de una tabla de búsqueda que almacena el número de ocurrencias para una determinada y única palabra.
Al trabajar sobre los tokens extraídos del código fuente bajo análisis, se construye  la \textit{tabla de frecuencias específica del programa}.
La segunda tabla, construída a partir de un conjunto mayor de programas, recibe el nombre de \textit{tabla de frecuencias global}.
Ambas son empleadas en la función de scoring para evaluar las posibles divisiones durante el proceso.

Por cada token a analizar, Samurai inicia ejecutando \textit{mixedCaseSplit}, que puede observarse en el Algoritmo \ref{mixedCaseSplit}.
El token original es dividido, añadiendo espacios en blanco en reemplazo de los caracteres especiales y, tanto antes como después, de cada conjunto de dígitos (línea 1).
Luego, se añade un espacio en blanco cuando encuentra una secuencia de dos caracteres en los cuales el primero es una minúscula y el segundo una mayúscula (línea 2).
En este punto, cada elemento tiene la forma de cero o más mayúsculas seguidas por cero o más minúsculas.
Después del procesamiento inicial, cada sub-cadena es examinada y evaluada para determinar si se continúa con la división camel-case regular, o si se opta por la alternativa.
Para esto, se determinan los puntajes para la separación por camel-case (líneas 9 y 11), y para la alternativa (línea 14).
Ambos puntajes son comparados (línea 16), y si hay evidencia suficiente que justifique la separación a la derecha del supuesto punto de corte, entonces la cadena de texto original es reemplazada por esta división (líneas 18 y 21).
Finalizados todos estos pasos, se llama a \textit{sameCaseSplit} por cada una de las sub-cadenas generadas (línea 29).

\begin{algorithm}[H]
\caption{mixedCaseSplit}
\label{mixedCaseSplit}
\DontPrintSemicolon
  \SetKwData{Token}{token}
  \SetKwData{SplitToken}{splitToken}
  \SetKwData{S}{s}
  \SetKwData{N}{n}
  \SetKwData{CamelScore}{camelScore}
  \SetKwData{AltScore}{altScore}
  
  \SetKwFunction{SplitChars}{splitOnSpecialCharactersAndDigits(\Token)}
  \SetKwFunction{SplitLowerUpperCase}{splitOnLowerCaseToUpperCase(\Token)}
  \SetKwFunction{IsUpper}{isUpper}
  \SetKwFunction{IsLower}{isLower}
  \SetKwFunction{Length}{length(\S)}
  \SetKwFunction{Score}{score}
  \SetKwFunction{ScoreS}{\Score(\S)}
  \SetKwFunction{SameCaseSplit}{sameCaseSplit(\S, \ScoreS)}

  \KwIn{\Token, para ser dividido}
  \KwOut{\SplitToken, token dividido delimitado por espacios}
  
  \BlankLine
  \Token $\leftarrow$ \SplitChars\;
  \Token $\leftarrow$ \SplitLowerUpperCase\;
  \SplitToken $\leftarrow$ $""$\;
  
  \BlankLine
  \ForEach{$\S \in \Token$}{
    \If{$\exists \{ i | \IsUpper(\S[i]) \land \IsLower(\S[i + 1])$\}}{
      \N $\leftarrow \Length - 1$\;
      
      \BlankLine
      \tcp{calcular el score para el split del camel case}
      \eIf{$i > 0$}{
        \CamelScore $\leftarrow \Score(\S[i, n])$\;
      }
      {
        \CamelScore $\leftarrow \Score(\S[0, n])$\;
      }
      
      \BlankLine
      \tcp{calcular el score alternativo}
      \AltScore $\leftarrow \Score(\S[i + 1, \N])$\;
      
      \BlankLine
      \tcp{elegir el split con base en el score}
      \eIf{$\CamelScore > \sqrt{\AltScore}$}{
        \If{$i > 0$}{
          \S $\leftarrow \S[0, i - 1] + "$ $" + \S[i, n]$\;
        }
      }{
        \S $\leftarrow \S[0, i] + "$ $" + \S[i + 1, n]$\;
      }
    }
    
    \BlankLine
    \SplitToken $\leftarrow \SplitToken + " " + \S$\;
  }

  \BlankLine
  \Token $\leftarrow$ \SplitToken\;
  \SplitToken $\leftarrow$ $""$\;
  
  \BlankLine
  \ForEach{$s_i \in \Token$}{
    \SplitToken $\leftarrow \SplitToken + "$ $" + \SameCaseSplit$\;
  }
  
  \BlankLine
  \KwRet \SplitToken\;
\end{algorithm}

Presentado en el Algoritmo \ref{sameCaseSplit}, \textit{sameCaseSplit} recibe como parámetros un string que puede estar en una de tres formas: a) mayúsculas, b) minúsculas, c) una mayúscula seguida de minúsculas; así como el scoring del token original.
Desde la posición inicial en el string (línea 3), el algoritmo analiza cada punto posible de separación en \textit{s}.
Para ello se calculan los scorings tanto para el lado izquierdo como para el derecho de este punto (líneas 6 y 7), y se evalúan dos condiciones para la separación: (a) si los substrings no son prefijos o sufijos comunes, (b) si hay suficiente evidencia como para soportar la división (líneas 11 y 12).
Si (a) se mantiene, pero sólo el substring de la izquierda provee suficiente evidencia para ser una palabra (línea 16), se llama recursivamente a \textit{sameCaseSplit} para determinar si existe una posible división dentro del fragmento de la derecha (línea 17).
Si para este substring se alcanza alguna separación válida, entonces tomamos como efectivas las divisiones de izquierda y derecha (línea 19).
De lo contrario, se descarta y continúa el proceso, ya que se podría estar realizando una división prematura y no deseada.

La raíz cuadrada del scoring aplicada en distintos puntos del algoritmo tiene la finalidad de reducir el impacto del puntaje de palabras cortas, ya que estas suelen tener mayor frecuencia y eso podría conducir a divisiones no relevantes, como por ejemplo \textit{per-formed} en lugar de \textit{performed}.

\begin{algorithm}[H]
\caption{sameCaseSplit}
\label{sameCaseSplit}
\DontPrintSemicolon
  \SetKwData{S}{s}
  \SetKwData{Score}{score}
  \SetKwData{ScoreNS}{$\Score_{ns}$}
  \SetKwData{SplitToken}{splitToken}
  
  \SetKwFunction{Length}{length}
  \SetKwFunction{FunScore}{score}
  \SetKwFunction{IsPrefix}{isPrefix}
  \SetKwFunction{IsSuffix}{isSuffix}
  \SetKwFunction{Max}{max}
  \SetKwFunction{SameCaseSplit}{sameCaseSplit}

  \KwIn{\S, string same-case}
  \KwIn{\ScoreNS, score para la NO división}
  \KwOut{\SplitToken, token dividido delimitado por espacios}
  
  \BlankLine
  \SplitToken $\gets$ \S\;
  $n \gets \Length(\S) - 1$\;
  $i \gets 0$\;
  $maxScore \gets -1$\;
  
  \BlankLine
  \While{$i < n$}{
    $score_l \gets \FunScore(\S[0, i])$\;
    $score_r \gets \FunScore(\S[i + 1, n])$\;
    $prefix \gets \IsPrefix(\S[0, i]) \lor \IsSuffix(\S[i + 1, n])$\;
    $toSplit_l \gets \sqrt{score_l} > \Max(\FunScore(\S), \ScoreNS)$\;
    $toSplit_r \gets \sqrt{score_r} > \Max(\FunScore(\S), \ScoreNS)$\;
    
    \BlankLine
    \uIf{$!prefix \land toSplit_l \land toSplit_r$}{
      \If{$(score_l + score_r) > maxScore$}{
        $maxScore \gets score_l + score_r$\;
        $\SplitToken \gets \S[0, i] + "$ $" + \S[i + 1, n]$\;
      }
    }
    \ElseIf{$!prefix \land toSplit_l$}{
      $temp \gets \SameCaseSplit(\S[i + 1, n], \ScoreNS)$\;
      \If{$temp.size > 1$}{
        $\SplitToken \gets \S[0, i] + "$ $" + temp$\;
      }
    }
    
    \BlankLine
    $i \gets i + 1$\;
  }
  
\end{algorithm}

Tanto \textit{mixedCaseSplit} como \textit{sameCaseSplit} se apoyan en la función de \textbf{scoring}.
Esta función determina una puntuación basada en qué tan frecuentemente aparece \textit{s} en el código fuente bajo análisis, y en un conjunto mayor de programas; y viene dada por:
\begin{center}
$Freq(s,p) + (globalFreq(s) / log_{10} (AllStrsFreq(p))$ 
\end{center}

Donde:
\begin{itemize}  \item $p \rightarrow$ programa bajo análisis.
  \item $Freq(s,p) \rightarrow$ frecuencia de \textit{s} en \textit{p}.
  \item $globalFreq(s) \rightarrow$ frecuencia de \textit{s} en un conjunto mayor de programas.
  \item $AllStrsFreq(p) \rightarrow$ frecuencia total de todos las cadenas de texto en \textit{p}.
\end{itemize}

En comparación con los otros algoritmos, Samurai performa de manera similar al algoritmo de división de tokens conservativo a la hora de atacar el problema de mixed-case.
En same-case, tiene la ventaja de que comete menos errores que Greedy y reduce drásticamente el \textit{oversplitting} (sobre-división), aunque a veces divide el token en menos partes que las esperadas.

\subsection{Algoritmo GenTest}
Este algoritmo, definido por Lawrie, Binkley y Morrell, se encarga de dividir eficientemente un indicador como parte de un proceso mayor dedicado a la normalización del vocabulario en el código fuente \cite{LawrieBinkleyMorrell2010}.
Recibe su nombre por la estrategia de \textit{generación} y \textit{prueba} que aplica.
Para la generacńo, simplemente se arman todas las divisiones posibles.
Si bien esto podría llevar a un número exponencial de opciones a explorar, la mayoría de las hardwords son palabras cortas y por lo tanto el esfuerzo computacional requerido no es excesivo.
La prueba es más compleja pero sin embargo más eficiente, ya que sólo consiste en aplicar una función de \textit{scoring} sobre cada división propuesta, y aquella que presente el mayor puntaje es la elegida.

El corazón de este algoritmo es una métrica de \textit{similaridad}, computada con base en los datos de ocurrencia.
Estos datos se utilizan porque se han probado útiles para resolver ambigüedades en traducciones, y se apoyan en la hipótesis de que softwords expandidas deberían encontrarse ubicadas cerca tanto en la documentación como en textos generales.

La función de scoring se compone de tres categorías de métricas.
\paragraph{Características de las softwords.}
Corresponde a las carasterísticas propias de cada cadena de texto generada en la división.
Dentro de estas métricas encontramos:
\begin{itemize}
  \item Número de palabras.
  \item Tamaño promedio de la palabra.
  \item Palabra más larga.
  \item Palabras con vocales.
  \item Número de palabras con un solo caracter.
\end{itemize}

\paragraph{Información externa.}
Incluye diccionarios y otra información que es provista tanto por humanos como fuentes que no son código.
Se utilizan dos diccionarios, en donde uno, el \textit{pequeño} es un subconjunto del diccionario \textit{grande}.
Además, dos métricas hacen uso de la información de co-ocurrencia desarrollada por Google, en donde se consulta un conjunto de datos que indica el número de veces que una serie de cinco palabras se observa en las páginas indexadas por el motor de búsqueda hasta 2006.
Las métricas que caben dentro de esta categoría son:
\begin{itemize}
  \item Cantidad de coincidencias en el diccionario pequeño.
  \item Cantidad de coincidencias en el diccionario grande.
  \item Cantidad de coincidencias en el diccionario grande con longitud 3.
  \item Cantidad de palabras de programación.
  \item Cantidad de expansiones del diccionario.
  \item Co-ocurrencia.
  \item Co-ocurrencia con longitud 3.
\end{itemize}

\paragraph{Información interna.}
Ésta se deriva del código fuente, así sea tanto del programa bajo análisis como de otros conjuntos de programas.
Además, se implementan también las tablas de frecuencia que son utilizadas en el Algoritmo Samurai, con la diferencia que dichas frecuencias se encuentran normalizadas.
En esta categoría las métricas son:
\begin{itemize}
  \item Presente en el código (ya que los caracteres forman un acrónimo para una frase encontrada en el código).
  \item Union-program probability (VER)
  \item Union-global probability (VER)
  \item Intersection-program probability (VER)
\end{itemize}

Como la variable de respuesta es binaria, es decir, si una división es correcta o incorrecta, se utilizó regresión logística para modelar la asociación entre la variable de respuesta y las variables explicativas.
El modelo resultante es una combinación ponderada de las métricas estadísticamente significativas, y es la función de scoring que se aplica para evaluar divisiones potenciales.

El algoritmo emplea una función \textit{Splits(id)}, la cual determina el conjunto de todas las posibles divisiones de un identificador \textit{id}.
Una división $s \in Splits(id)$ está compuesta por una serie de softwords $s_1 s_2 \dots s_n$.
Para cada softword $s_i$, la función $E(s_i)$ establece el conjunto de las distintas expansiones posibles $e_{i,1}, e_{i,2}, \dots, e_{i,m}$ para $s_i$.
En el algoritmo, la similitud entre dos expansiones, $sim(e_1, e_2)$ viene dada por la probabilidad de que $e_1$ y $e_2$ co-ocurran en una ventana de cinco palabras en el conjunto de datos de Google.

Para una división $s = s_1 s_2 \dots s_n$, por cada expansión $e_{i,j} \in E(s_i)$, se define el puntaje de la similiridad entre $e_{i,j}$ y las otras softwords de $s$, donde $s_k (k \neq i)$ como la suma de las similaridades entre $e_{i,j}$ y los elementos de $E(s_k)$:

\begin{center}
  $\underset{(k \neq i)}{sim(e_{i,j}, s_k)} = \displaystyle\sum_{e \in E(s_k)} sim(e_{i,j}, e)$
\end{center}

La \textit{cohesión} para $e_{i,j}$ relativa a $s$ se define como:

\begin{center}
  $cohesion(e_{i,j}, s) = log \displaystyle [\sum_{s_k \in s \neq s_i} sim(e_{i,j}, s_k)]$
\end{center}

Por cada $s_i \in s$, $score(s_i)$ es la cohesión de la expansión $e_{i,j} \in E(s_i)$ que tiene el valor máximo de \textit{cohesion}:

\begin{center}
  $score(s_i) = max_{e_{i,j} \in E(s_i)} [cohesion(e_{i,j}, s)]$
\end{center}

En comparación con Greedy y Samurai, el algoritmo GenTest performa mucho mejor, con un nivel de precisión mayor.
En los casos en los que no logra obtener el resultado deseado, se observa que este algoritmo tiende a realizar menos divisiones que las esperadas (\textit{undersplit}).
