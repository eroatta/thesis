\chapter{Análisis de Identificadores}

\section{Identificadores}
Formalmente, un identificador t es una secuencia de caracteres c0, c1, …, cn, dondel el caracter ci representa una letra, un dígito o un caracter especial. Hard words, soft words. COMPLETAR

Ver Rilling y Klemola - Identifying comprehension bottlenecks using program slicing and cognitive complexity metrics.

Ver Caprille y Tonella - Reestructuring program identifier names.

\subsection{Identificadores concisos y consistentes}
Cada lenguaje de programación plantea sus propios estilos de codificación, así como también convenciones para los nombres de los identificadores. Sin embargo, éstas suelen enfocarse solamente en la sintaxis, dejando de lado uno de los aspectos de mayor interés, el cual se corresponde a la semántica de los nombres empleados. La importancia en la definición de nombres adecuados radica en la influencia que los mismos tienen durante la comprensión de programas, y como consecuencia, en la calidad y productividad a lo largo del ciclo de vida del software. La ofuscación de código a través de la conversión de identificadores a secuencias aleatorias de caracteres es un claro ejemplo del impacto que tienen los nombres en el proceso de comprensión.

DeiBenbock y Pizka definen, a través de un \textbf{modelo formal}, una estrategia para la creación de nombres bien formados. En ésta se incluyen dos reglas, las cuales permiten un nombramiento conciso y consistente de variables, funciones y clases \cite{DeiBenbockPizka05}. La premisa sobre la que basan estas reglas viene dada por la correlación de identificadores con el conjunto de conceptos que son utilizados en un programa.

\subsubsection{Conceptos nombrados}
Primero, se establece a \textit{C} como el conjunto de todos los conceptos relevantes dentro de un determinado alcance (un componente de un programa, el dominio de la aplicación o una organización). A su vez, se define un concepto como una unidad con un significado asociado en términos de comportamiento o propiedades. Se modelan también todos los nombres posibles y se denotan por \textit{N}, junto con la asignación de nombres a conceptos como una relación formal $R \subseteq N \times C$.

\subsubsection{Regla 1: Consistencia}
Del lenguaje natural heredamos dos tipos de incosistencias: homónimos y sinónimos.

Un \textbf{homónimo} es aquella palabra que se escribe o pronuncia igual que otra, pero tiene diferente significado, y su definición formal viene dada por: un nombre $n \in N$ es llamado homónimo sí y solo sí $|C_n| > 1$ donde $C_n = \{ c \in C : (n,c) \in R\}$.

A su vez, un \textbf{sinónimo} es aquella palabra que, si bien se escribe de forma diferente, refiere al mismo concepto que otra. La definición formal se establece como: dos nombres $s,n \in N$ son sinónimos sí y sólo sí $C_s \cap C_n \neq 0$.

Ante la ausencia de homónimos, el daño de los sinónimos es limitado ya que siempre apuntan a un único concepto. Sin embargo, incrementan innecesariamente el dominio \textit{N} y la relación \textit{R}, y por consiguiente aumentan el esfuerzo para comprender el lenguaje utilizado. La presencia de homónimos y sinónimos tiene un impacto negativo importante ya que para cada identificador \textit{n}, el desarrollador tiene que considerar todos los conceptos en
\\
\\$M_n = \bigcup_{e \in n} C_e$
\\
\\donde $S_n$ es el conjunto de todos los nombres sinónimos a \textit{n} (incluyendo a \textit{n} mismo).

Dadas estas condiciones, podemos decir que un sistema de nombres \textit{C}, \textit{N} y \textit{R} es \textit{consistente} sí y sólo sí $R \subseteq N \times C$ es una relación bijectiva. Por lo tanto, se define como:
\\$n : C \rightarrow N$
\\$n(c) = \mbox{nombre único del concepto} \ c$.

\subsubsection{Regla 2: Conciso}
Para definir \textit{conciseness} debemos introducir el orden parcial $\sqsubset$ para el conjunto de conceptos \textit{C}, de acuerdo a su nivel de abstracción.

Dado el conjunto \textit{P}, el cual contiene elementos del programa que son identificados como unidades a través de un nombre simbólico, y dada \textit{i}, que representa el mapeo de los elementos del programa con sus identificadores.
\\$i : P \rightarrow N$
\\$i(p) = \mbox{identificador de} \ p$.

Además, dado $\lbrack c \rbrack$, el cual denota la semántica en el sentido del significado de un concepto $c \in C$. De la misma manera, $\lbrack p \rbrack$ denota la semántica de un elemento del programa \textit{p}.

Establecidas estas condiciones, definimos el problema de \textit{conciseness} en dos pasos. Primero se requiere la \textit{correcta} identificación, y luego la validación de \textit{conciseness}.

\paragraph{Definición: correctitud}
Sea $p \in P$ un elemento del programa y $c \in C$ el concepto que implementa, tal que $\lbrack p \rbrack = \lbrack c \rbrack$. El identificador \textit{i(p)} es \textit{correcto} sí y solo sí ocurre lo siguiente:
\\$i(p) \in \lbrace n(c') : c' \in C \land c' \sqsupseteq c \rbrace$
\\Esto significa que el identificador de un elemento del programa \textit{p} que manifiesta el concepto \textit{c} debe corresponderse al nombre de \textit{c} o a una generalización del mismo. Sin embargo, a veces ocurre que de alguna manera, los nombres de los identificadores son correctos pero no lo suficientemente consisos. Para contrarestar esta situación, se agrega el siguiente requerimiento:

\paragraph{Definición: conciso}
Sea $p \in P$ un elemento del programa y $c \in C$ el concepto que implementa, tal que $\lbrack p \rbrack = \lbrack c \rbrack$. El identificador \textit{i(p)} para el elemento \textit{p} del programa es \textit{conciso} sí y sólo sí lo siguiente es verdad:
\\$i(p) = n(c)$
\\Esta definición requiere que un identificador tenga exactamente el mismo nombre del concepto que representa.

INSERTAR IMAGENES

\subsubsection{Consistencia y Conciseness sintácticas}
El modelo formal descrito por DeiBenbock y Pizka requiere que exista un mapeo entre los elementos del dominio de conceptos y los identificadores. Sin esta definición, se hace imposible la relación entre los dos conjuntos. Ahora bien, para nuevos programas, la construcción de este mapeo puede hacerse al mismo tiempo que el desarrollo con un mínimo costo extra. Sin embargo, para aquellos proyectos existentes, el costo puede ser demasiado. Para contrarestar esta limitación, se plantea el uso de las \textit{consistencia y conciseness sintáticas}, en donde sólo se considera la construcción sintática de los identificadores \cite{LawrieFeildBinkley06}.

El enfoque se basa en la contención de identificadores. Se dice que un identificador está contenido dentro de otro si todas sus \textit{soft words} están presentes, en el mismo orden, en el identificador contenedor. Cuando un identificador está contenido dentro de otro, una de dos posibles violaciones han ocurrido. Por un lado, puede que haya un solo concepto asociado a dos identificadores, lo que implica una violación al requerimiento respecto a los sinónimos en la consistencia; por otro, que los dos identificadores refieran a diferentes conceptos. En este caso, no se cumpliría la regla de nombres concisos.

INSERTAR IMAGENES

\paragraph{Definición: Conciseness y Consistencia de sinónimos sintática}
Sea el identificador $id_1$ una secuencia de soft words $sw_1 sw_2 ... sw_n1$. Los identificadores $id_1$ e $id_2$ no cumplen el \textit{requerimiento sobre sinónimos en la consistencia sintáctia} si $id_2$ incluye la secuencia de soft words $w_1 w_2 ... sw_1 sw_2 ... sw_n1 ... w_n2$. Además, $id_1$ falla el \textit{requerimiento sobre la concisenss sintáctica} si existe un tercer identificador $id_3$ que incluya la secuencia de soft words $u_1 u_2 ... sw_1 sw_2 ... sw_n1 ... u_n3$.

\section{División}
El primer paso para analizar los identificadores que se utilizan consiste en dividir cada uno de ellos, en los elementos que lo conforman. Los desarrolladores componen estos identificadores concatenando palabras y abreviaciones, a veces, empleando convenciones claras en la demarcación de sus partes, como lo son el uso de caracteres no alfabéticos (por ejemplo: ``\_'' o ``-''), o la técnica camel-case, donde la primera letra de cada palabra se escribe en mayúsculas a excepción de la inicial. Esta situación permite que la división automática se lleve a cabo sin ningún problema. Sin embargo, en los casos en los que no se implementa una clara delimitación es donde surge la necesidad de aplicar técnicas más sofisticadas para lograr la separación de las partes. Aquí es donde entran en juego los \textbf{algoritmos de división}.
 
El objetivo de un algoritmo divisor de identificadores es tomar uno de estos últimos como entrada, y obtener como salida una lista de sub-elementos que particionan al identificador original. Estos sub-elementos pueden ser palabras de diccionario, las cuales tienen un significado obvio; abreviaturas, las cuales refieren a una sola palabra del diccionario; o acrónimos, los cuales representan varias palabras de diccionario \cite{HillBinkleyLawrie14}.

\subsection{El problema de la división de tokens}
Ahora bien, estos algoritmos deben ser capaces de afrontar una serie de problemas relacionados a la correcta división de los identificadores. Como se dijo anteriormente, una forma de establecer una separación entre las palabras componentes de un indicador es el uso de caracteres especiales, o a través de la técnica de \textit{camel-casing}. Esta última permite reducidir la cantidad de caracteres sin comprometer la legibilidad, aunque existen situaciones donde no hay convenciones establecidas y esto puede afectar negativamente la comprensión, siendo ejemplos de este caso la incorporación de acrónimos al identificador (ej: \textit{sqlList, SQLList, SQLlist}) o la no utilización de delimitadores para conceptos multi palabras que son de uso común, como en \textit{sizeof} o \textit{hostname}.

Un token puede ser definido formalmente como $t = (s_0, s_1, s_2, ..., s_n)$ donde $s_i$ es una letra, un dígito o un caracter especial. El primer paso consiste en separar el token a través utilizando los dígitos y caracteres especiales como marcadores. Cada sub-string es considerado como un token potencialmente divisible, recibe el nombre de \textit{token alfabético}, y existen cuatro casos posibles a la hora de decidir si dividirlo en un cierto punto entre $s_i$ y $s_j$\cite{EnslenHillPollock09}:

\begin{enumerate}
  \item $s_i$ está en minúsculas y $s_j$ en mayúsculas (ej: getString, setPoint)
  \item $s_i$ está en mayúsculas y $s_j$ en minúsculas (ej: getMAXstring, GPSstate, ASTVisitor)
  \item tanto $s_i$ como $s_j$ están en minúsculas (ej: notype, databasefield, actionparameters)
  \item tanto $s_i$ como $s_j$ están en mayúsculas (ej: USERLIB, NONNEGATIVEDECIMALTYPE, COUNTRYCODE)
\end{enumerate}

El primer caso, es el lugar natural para realizar la división. En el segundo caso, en el cual el lugar donde dividir existen alternaciones de mayúsculas y minúsculas, presenta el problema de la separación de tokens \textit{mixed-case}, particularmente complicado por la utilización de acrónimos. Por último, los casos 3 y 4 corresponden al problema de la separación de tokens \textit{same-case}; en donde los programadores no han dejado rastro donde cualquier palabra o concepto deba ser extraída del token.

Un algoritmo de división de tokens completamente automático debería ser capaz de resolver los sub-problemas de mixed-case y same-case, de forma efectiva.

\subsection{Estado del arte}
De acuerdo a la recopilación de Hill et. all (Cómo se pone?), algunas de las principales técnicas para la división de identificadores son:

\begin{itemize}
  \item \textbf{Greedy.} Esta técnica utiliza un diccionario, una lista de abreviaturas conocidas y una lista de corte, la cual incluye identificadores predefinidos, liberías, funciones, nombres de variables comunes y letras individuales. Después de retornar cada hard word encontrada en alguna de las tres listas, como una soft word simple, el resto de las hard words se consideran para división. A partir de ahí, recursivamente, se analizan los sufijos y prefijos de las palabras hasta que se encuentren en alguna de las listas, prefiriendo siempre las palabras de mayor longitud.
  
  \item \textbf{Samurai.} Este algoritmo se basa en la premisa de que las partes que componen a los identifcadores multi-palabras de un determinado programa, suelen ser utilizados en algún otro lugar, así sea en el mismo software como en otros. Por lo tanto, la frecuencia de aparición de las palabras es el principal elemento considerado a la hora de dividir identificadores.
  
  \item \textbf{GenTest.} Tiene su fuerte en la división de términos same-case, generando primero todas las posibles divisiones (dado que los identificadores son relativamente cortos, el potencialmente exponencial número de fragmentos es manejable en la práctica), para luego aplicar un \textit{scoring} sobre cada elemento, seleccionando la de mayor valor. Esta función de scoring se apoya en que las soft words iguales o similares, deberían encontrarse ubicadas cerca una de otra en la documentación o texto general (métrica de similaridad). 
  
  \item \textbf{DTW.} Esta técnica se basa en la observación de que los programadores construyen nuevos identificadores aplicando un conjunto de transformaciones a las palabras, como por ejemplo quitar todas las vocales o algunos caracteres. Utilizando un diccionario que contenga palabras y terminos pertenecientes a una ontología superior, al dominio de la aplicación o ambos, el objetivo consiste en identificar un matcheo cuasi-óptimo entre las partes del identificador y las palabras en el diccionario.
  
  \item \textbf{INTT.} El enfoque INTT busca realizar una división más precisa que las técnicas previas al utilizar una heurística especializada para manejar identificadores con dígitos, sin aplicar la separación de los digitos del resto del texto en etapas tempranas del proceso de división. Las principales modificaciones consisten en reemplazar greedy por dos algoritmos, greedy y greedier, los cuales pueden reconocer las partes de los identificadores same-case, sin requerir que comiencen o terminen con una palabra conocida; y además utilizar una lista de acrónimos que contiene dígitos.
\end{itemize}

Para el presente informe, sólo son de interés - y por lo tanto explicadas en mayor detalle - las primeras tres técnicas listadas anteriormente.

\subsection{Algoritmo Greedy}
Desarrollado por Feild, Binkley and Lawrie \cite{FieldBinkleyLawrie06, Feild06anempirical, Lawrie2007, Lawrie:2007:EMA:1306878.1307350, EnslenHillPollock09}, este algoritmo recibe su nombre gracias al enfoque que toma para resolver el problema, eligiendo en cada paso local la opción más conveniente con la idea de lograr, a nivel general, una solución óptima. El proceso comienza realizando una búsqueda por cada hard word dentro de un conjunto de listas de palabras y abreviaturas. Si la palabra candidata existe en alguna de ellas, se devuelve como válida, si no, se asume que se compone de múltiples palabras, y éstas son tratadas recursivamente hasta encontrar sus componentes \cite{Feild06anempirical}. Las tres listas que se utilizan son:

\begin{itemize}
  \item \textbf{Diccionario:} se conforma con diccionarios de dominio público, los cuales son consultados en la herramienta de verificación de gramática \textit{ispell}, disponible en Linux.
  
  \item \textbf{Abreviaturas conocidas:} consiste de abreviaturas que son de uso común y relacionadas tanto al dominio del problema como a la programación.
  
  \item \textbf{Stop-list:} es la lista de corte y está compuesta de tres tipos de palabras, dentro de las cuales encontramos a los tipos de datos predefinidos, variables globales (tanto del lenguage como del ambiente), y los nombres de librerías estándar.
\end{itemize}

Tal como puede verse en el algorimo \ref{algGreedy}, el proceso comienza dividiendo el token de entrada en hardwords, tomando como delimitadores a los caracteres especiales y la técnica de \textit{camel-casing}.
Una vez hecho tanto el reemplazo de números y caracteres especiales (línea 4), como la división inicial del token (línea 5), se procede con la evaluación de cada una de las hardwords resultantes.
En primera instancia, se verifica si la palabra candidata existe en alguna de las listas con las que trabaja el algoritmo (línea 8). Si es así, el fragmento del token es considerado parte de la división y por lo tanto de la salida del proceso (línea 13).
En caso contrario, al asumirse que está formado por múltiples palabras, se procesa en búsqueda de las softwords que lo componen.
Este análisis consiste en realizar dos búsquedas sobre cada hardword, quedándose con el mejor resultado entre ambas.
En la primera búsqueda (línea 9) se trata de obtener el prefijo más largo que se encuentre en una de las listas, y forme parte de la palabra actual. Luego se llama recursivamente, trabajando sobre el fragmento restante de la palabra original (prefijos y sufijos).
La segunda búsqueda (línea 10) es idéntica, excepto que trata de obtener el sufijo más largo.
Los resultados de ambos procesamientos son evaluados, y aquel que arroje la relación más alta entre la cantidad de softwords encontradas en listas vs. el total de palabras, es el elegido (línea 11).
Al finalizar la ejecución del algoritmo, la salida que se obtiene es el identificador original dividido, separando cada una de los componentes encontrados, a través de espacios en blanco (línea 16).

\begin{algorithm}[H]
\caption{Greedy}
\label{algGreedy}
\DontPrintSemicolon
  \SetKwData{Token}{token}
  \SetKwData{SplitToken}{splitToken}
  \SetKwData{Dictionary}{dictionary}
  \SetKwData{KnownAbbrs}{knownAbbreviations}
  \SetKwData{StopList}{stopList}
  \SetKwData{Preffix}{preffix}
  \SetKwData{Suffix}{suffix}
  
  \SetKwFunction{SplitChars}{splitOnSpecialCharactersAndDigits(\Token)}
  \SetKwFunction{SplitLowerUpperCase}{splitOnLowerCaseToUpperCase(\Token)}
  \SetKwFunction{FindPreffix}{findPreffix($s_i$,$""$)}
  \SetKwFunction{FindSuffix}{findSuffix($s_i$,$""$)}
  \SetKwFunction{Compare}{compare(\Preffix,\Suffix)}
  
  \KwIn{\Token, para ser divido}
  \KwOut{\SplitToken, token dividido delimitado por espacios}
  
  \BlankLine
  var \Dictionary\;
  var \KnownAbbrs\;
  var \StopList\;
  
  \BlankLine
  \Token $\leftarrow$ \SplitChars\;
  \Token $\leftarrow$ \SplitLowerUpperCase\;
  \SplitToken $\leftarrow$ $""$\;
  
  \BlankLine
  \ForEach{$s_i \in \Token$}{
    \eIf{$s_i \not \in (\Dictionary \cup \KnownAbbrs \cup \StopList)$}{
      \Preffix $\leftarrow$ \FindPreffix\;
      \Suffix $\leftarrow$ \FindSuffix\;
      \SplitToken $\leftarrow$ \Compare\;
    }
    {\SplitToken $\leftarrow$ $s_i$}
  }
  \BlankLine
  \KwRet \SplitToken\;
\end{algorithm}

La función \textit{findPreffix} es descrita en el algoritmo \ref{funFindPreffix}. 
La primer validación que se realiza sirve como punto de corte para la recursión (línea 1), cuando el token a analizar está vacío.
Si el parámetro recibido es parte de alguna de las listas (línea 4), la función finaliza separando el prefijo encontrado y llamándose recursivamente con el fragmento restante (línea 5).
En caso contrario, el último caracter es removido (línea 8) y el proceso continúa llamándose a si mismo (línea 9).
Los caracteres removidos son agrupados (línea 7) y luego agregados al resultado. 
Si la primera palabra en el identificador dividido no está en una de las listas, los caracteres se van acoplando. Si lo está, se agregan como una nueva softword.

\begin{algorithm}
\caption{Función findPreffix}
\label{funFindPreffix}
\DontPrintSemicolon
  \SetKwData{S}{s}
  \SetKwData{SSplit}{sSplit}
  \SetKwData{Dictionary}{dictionary}
  \SetKwData{KnownAbbrs}{knownAbbreviations}
  \SetKwData{StopList}{stopList}
  
  \SetKwFunction{Size}{size}
  \SetKwFunction{FindPreffixSSplitEmpty}{findPreffix(\SSplit,$""$)}
  \SetKwFunction{FindPreffixSSSplit}{findPreffix(\S,\SSplit)}
  
  \KwIn{\S, string para ser analizado}
  \KwOut{\SSplit, string dividido y delimitado por espacios}
  
  \BlankLine
  \If{$\S.\Size == 0$}{
    \KwRet \SSplit
  }
  
  \BlankLine
  \If{$\S \in (\Dictionary \cup \KnownAbbrs \cup \StopList)$}{
    \KwRet $\S + "" + \FindPreffixSSplitEmpty$ 
  }
  
  \BlankLine
  \SSplit $\leftarrow \S[length(\S) - 1] + \SSplit$\;
  
  \BlankLine
  \S $\leftarrow \S[0, length(\S) - 1]$\;
  
  \BlankLine
  \KwRet \FindPreffixSSSplit
\end{algorithm}

Una lógica similar se sigue para la función \textit{findSuffix}, visible en el algoritmo \ref{funFindSuffix}. 
Cuando el parámetro se encuentra de alguna de las listas (línea 4), el sufijo se extrae y la función continúa llamándose recursivamente (línea 5).
En caso contrario, el primer caracter es removido (línea 8) y el proceso continúa llamándose a si mismo (línea 9), reduciendo la longitud del sufijo a evaluar.
Al igual que en la otra función, los caracteres removidos son agrupados (línea 7) y luego agregados al resultado.

\begin{algorithm}
\caption{Función findSuffix}
\label{funFindSuffix}
\DontPrintSemicolon
  \SetKwData{S}{s}
  \SetKwData{SSplit}{sSplit}
  \SetKwData{Dictionary}{dictionary}
  \SetKwData{KnownAbbrs}{knownAbbreviations}
  \SetKwData{StopList}{stopList}
  
  \SetKwFunction{Size}{size}
  \SetKwFunction{FindSuffixSSplitEmpty}{findSuffix(\SSplit,$""$)}
  \SetKwFunction{FindSuffixSSSplit}{findSuffix(\S,\SSplit)}
  
  \KwIn{\S, string para ser analizado}
  \KwOut{\SSplit, string dividido y delimitado por espacios}
  
  \BlankLine
  \If{$\S.\Size == 0$}{
    \KwRet \SSplit
  }
  
  \BlankLine
  \If{$\S \in (\Dictionary \cup \KnownAbbrs \cup \StopList)$}{
    \KwRet $\FindSuffixSSplitEmpty + "" + \S$ 
  }
  
  \BlankLine
  \SSplit $\leftarrow \SSplit + \S[0] $\;
  
  \BlankLine
  \S $\leftarrow \S[1, length(\S)]$\;
  
  \BlankLine
  \KwRet \FindSuffixSSSplit
\end{algorithm}

Este algoritmo, en comparación con otros existentes, tiene la ventaja de que es independiente del conjunto de datos sobre el que se ejecuta. Además, su lógica e implementación, son sencillas, lo que lo convierte en el punto de comparación a la hora de evaluar la performance de otras propuestas.

Sin embargo, tiene algunos aspectos negativos. El tiempo de ejecución puede ser elevado, ya que la inicialización está dominada por la carga de varios diccionarios en tablas \textit{hash} y por la utilización de recursividad \cite{FieldBinkleyLawrie06}.
Otra desventaja que presenta es que tiende a realizar una mayor cantidad de divisiones que las esperadas \cite{Feild06anempirical}.

\subsection{Algoritmo Samurai}
Propuesto por Hill et al., este algoritmo está inspirado en un trabajo previo, sobre el minado automático de código fuente para la expansión de abreviaturas \cite{Hill:2008:AAM:1370750.1370771}.
Las hipótesis sobre las que basan la lógica del proceso son dos. 
En primera instancia, los strings que componen los tokens multi-palabras tienden a ser empleados en alguna otra parte del código, tanto del mismo programa como de otros. Estos tokens pueden aparecer tanto como una palabra independiente, o como parte de una composición.
En segunda instancia, se apoyan en la hipótesis de que las divisiones suelen ser en favor de aquellos tokens que ocurren más a menudo en un programa. Por lo tanto, la frecuencia de una palabra se utiliza para determinar los cortes en el token analizado.

Para poder llevar adelante la división, es necesario explorar el conjunto de tokens existentes en el código y crear dos tablas de frecuencia.
Este proceso de minado se realiza ejecutando el algoritmo de división de tokens conservativo basado en marcadores de separación (VER) sobre un conjunto de tokens del código fuente, para así generar un listado de todas las ocurrencias de los strings delimitados por marcadores.
Este listado es luego procesado para obtener la información relativa de frecuencias, en la forma de una tabla de búsqueda que almacena el número de ocurrencias para una determinada y única palabra.
Al trabajar sobre los tokens extraídos del código fuente bajo análisis, se construye  la \textit{tabla de frecuencias específica del programa}.
La segunda tabla, construída a partir de un conjunto mayor de programas, recibe el nombre de \textit{tabla de frecuencias global}.
Ambas son empleadas en la función de scoring para evaluar las posibles divisiones durante el proceso de separación.

Por cada token a analizar, Samurai inicia ejecutando \textit{mixedCaseSplit}, que puede observarse en el Algoritmo \ref{mixedCaseSplit}.
El token original es dividido, añadiendo espacios en blanco en reemplazo de los caracteres especiales y, tanto antes como después, de cada conjunto de dígitos (línea 1).
Luego, se añade un espacio en blanco cuando encuentra una secuencia de dos caracteres en los cuales el primero es una minúscula y el segundo una mayúscula (línea 2). En este punto, cada elemento tiene la forma de cero o más mayúsculas seguidas por cero o más minúsculas.
Después del procesamiento inicial, cada substring es examinado y evaluado para determinar si se continúa con la división camel-case regular, o si se opta por la alternativa. Para esto, se determinan los scores para la separación por camel-case (líneas 9 y 11), y para la alternativa (línea 14).
Ambas evaluaciones son comparadas (línea 16), y si hay evidencia suficiente que justifique la separación a la derecha del supuesto punto de corte, entonces el string original es reemplazado por esta división (líneas 18 y 21).
Finalizados todos estos pasos, se llama a \textit{sameCaseSplit} por cada uno de las subcadenas generadas (línea 29).

\begin{algorithm}[H]
\caption{mixedCaseSplit}
\label{mixedCaseSplit}
\DontPrintSemicolon
  \SetKwData{Token}{token}
  \SetKwData{SplitToken}{splitToken}
  \SetKwData{S}{s}
  \SetKwData{N}{n}
  \SetKwData{CamelScore}{camelScore}
  \SetKwData{AltScore}{altScore}
  
  \SetKwFunction{SplitChars}{splitOnSpecialCharactersAndDigits(\Token)}
  \SetKwFunction{SplitLowerUpperCase}{splitOnLowerCaseToUpperCase(\Token)}
  \SetKwFunction{IsUpper}{isUpper}
  \SetKwFunction{IsLower}{isLower}
  \SetKwFunction{Length}{length(\S)}
  \SetKwFunction{Score}{score}
  \SetKwFunction{ScoreS}{\Score(\S)}
  \SetKwFunction{SameCaseSplit}{sameCaseSplit(\S, \ScoreS)}

  \KwIn{\Token, para ser dividido}
  \KwOut{\SplitToken, token dividido delimitado por espacios}
  
  \BlankLine
  \Token $\leftarrow$ \SplitChars\;
  \Token $\leftarrow$ \SplitLowerUpperCase\;
  \SplitToken $\leftarrow$ $""$\;
  
  \BlankLine
  \ForEach{$\S \in \Token$}{
    \If{$\exists \{ i | \IsUpper(\S[i]) \land \IsLower(\S[i + 1])$\}}{
      \N $\leftarrow \Length - 1$\;
      
      \BlankLine
      \tcp{calcular el score para el split del camel case}
      \eIf{$i > 0$}{
        \CamelScore $\leftarrow \Score(\S[i, n])$\;
      }
      {
        \CamelScore $\leftarrow \Score(\S[0, n])$\;
      }
      
      \BlankLine
      \tcp{calcular el score alternativo}
      \AltScore $\leftarrow \Score(\S[i + 1, \N])$\;
      
      \BlankLine
      \tcp{elegir el split con base en el score}
      \eIf{$\CamelScore > \sqrt{\AltScore}$}{
        \If{$i > 0$}{
          \S $\leftarrow \S[0, i - 1] + "$ $" + \S[i, n]$\;
        }
      }{
        \S $\leftarrow \S[0, i] + "$ $" + \S[i + 1, n]$\;
      }
    }
    
    \BlankLine
    \SplitToken $\leftarrow \SplitToken + " " + \S$\;
  }

  \BlankLine
  \Token $\leftarrow$ \SplitToken\;
  \SplitToken $\leftarrow$ $""$\;
  
  \BlankLine
  \ForEach{$s_i \in \Token$}{
    \SplitToken $\leftarrow \SplitToken + "$ $" + \SameCaseSplit$\;
  }
  
  \BlankLine
  \KwRet \SplitToken\;
\end{algorithm}

Presentado en el Algoritmo \ref{sameCaseSplit}, \textit{sameCaseSplit} recibe como parámetros un string que puede estar en una de tres formas: a) mayúsculas, b) minúsculas, c) una mayúscula seguida de minúsculas; así como el scoring del token original.
Desde la posición inicial en el string (línea 3), el algoritmo analiza cada punto posible de separación en \textit{s}.
Para ello se calculan los scorings tanto para el lado izquierdo como para el derecho de este punto (líneas 6 y 7), y se evalúa dos condiciones para la separación: (a) los substrings no son prefijos o sufijos comunes, (b) hay suficiente evidencia como para soportar la división (líneas 11 y 12).
Si (a) se mantiene, pero sólo el substring de la izquierda provee suficiente evidencia para ser una palabra (línea 16), se llama recursivamente a \textit{sameCaseSplit} para determinar si existe una posible división dentro del fragmento de la derecha (línea 17).
Si para este substring se alcanza alguna separación válida, entonces tomamos como efectivas las divisiones de izquierda y derecha (línea 19). De lo contrario, se descarta y continúa el proceso, ya se podría estar realizando una división prematura y no buscada.

La raíz cuadrada del scoring aplicada en distintos puntos del algoritmo tiene la finalidad de reducir el impacto del score de palabras cortas, ya que estas suelen tener mayor frecuencia y eso podría conducir a divisiones no relevantes, como por ejemplo \textit{per-formed} en lugar de \textit{performed}.

\begin{algorithm}[H]
\caption{sameCaseSplit}
\label{sameCaseSplit}
\DontPrintSemicolon
  \SetKwData{S}{s}
  \SetKwData{Score}{score}
  \SetKwData{ScoreNS}{$\Score_{ns}$}
  \SetKwData{SplitToken}{splitToken}
  
  \SetKwFunction{Length}{length}
  \SetKwFunction{FunScore}{score}
  \SetKwFunction{IsPrefix}{isPrefix}
  \SetKwFunction{IsSuffix}{isSuffix}
  \SetKwFunction{Max}{max}
  \SetKwFunction{SameCaseSplit}{sameCaseSplit}

  \KwIn{\S, string same-case}
  \KwIn{\ScoreNS, score para la NO división}
  \KwOut{\SplitToken, token dividido delimitado por espacios}
  
  \BlankLine
  \SplitToken $\gets$ \S\;
  $n \gets \Length(\S) - 1$\;
  $i \gets 0$\;
  $maxScore \gets -1$\;
  
  \BlankLine
  \While{$i < n$}{
    $score_l \gets \FunScore(\S[0, i])$\;
    $score_r \gets \FunScore(\S[i + 1, n])$\;
    $prefix \gets \IsPrefix(\S[0, i]) \lor \IsSuffix(\S[i + 1, n])$\;
    $toSplit_l \gets \sqrt{score_l} > \Max(\FunScore(\S), \ScoreNS)$\;
    $toSplit_r \gets \sqrt{score_r} > \Max(\FunScore(\S), \ScoreNS)$\;
    
    \BlankLine
    \uIf{$!prefix \land toSplit_l \land toSplit_r$}{
      \If{$(score_l + score_r) > maxScore$}{
        $maxScore \gets score_l + score_r$\;
        $\SplitToken \gets \S[0, i] + "$ $" + \S[i + 1, n]$\;
      }
    }
    \ElseIf{$!prefix \land toSplit_l$}{
      $temp \gets \SameCaseSplit(\S[i + 1, n], \ScoreNS)$\;
      \If{$temp.size > 1$}{
        $\SplitToken \gets \S[0, i] + "$ $" + temp$\;
      }
    }
    
    \BlankLine
    $i \gets i + 1$\;
  }
  
\end{algorithm}

Tanto \textit{mixedCaseSplit} como \textit{sameCaseSplit} se apoyan en la función de \textbf{scoring}.
Esta función determina una puntuación basada en qué tan frecuentemente aparece \textit{s} en el código fuente bajo análisis, y en un conjunto mayor de programas; y viene dada por:
\begin{center}
$Freq(s,p) + (globalFreq(s) / log_{10} (AllStrsFreq(p))$ 
\end{center}

Donde:
\begin{itemize}  \item $p \rightarrow$ programa bajo análisis.
  \item $Freq(s,p) \rightarrow$ frecuencia de \textit{s} en \textit{p}.
  \item $globalFreq(s) \rightarrow$ frecuencia de \textit{s} en un conjunto mayor de programas.
  \item $AllStrsFreq(p) \rightarrow$ frecuencia total de todos las cadenas de texto en \textit{p}.
\end{itemize}

En comparación con los otros algoritmos, Samurai performa de manera similar a conserv(REFERENCIA) a la hora de atacar el problema de mixed-case.
En same-case, tiene la ventaja de que comete menos errores que Greedy y reduce drásticamente el \textit{oversplitting}, aunque a veces divide el token en menos partes que las esperadas.

\subsection{Algoritmo GenTest}
Este algoritmo, definido por Lawrie, Binkley y Morrell, se encarga de dividir eficientemente un indicador como parte de un proceso mayor dedicado a la normalización del vocabulario en el código fuente \cite{LawrieBinkleyMorrell2010} INSERTAR LA OTRA REF.
Recibe su nombre por la estrategia de \textit{generación} y \textit{prueba} que aplica.
Para la generación, simplemente se arman todas las divisiones posibles.
Si bien esto podría generar un número exponencial de opciones a explorar, la mayoría de las hard words son palabras cortas y por lo tanto el esfuerzo computacional requerido no es excesivo.
La prueba es más compleja pero sin embargo más eficiente, ya que sólo consiste en aplicar una función de \textit{scoring} sobre cada división propuesta, y aquella que presente el mayor puntaje es la elegida.

El corazón de este algoritmo es una métrica de \textit{similaridad}, computada con base en los datos de ocurrencia.
Estos datos se utilizan porque se han probado útiles para resolver ambig:uedades en traducciones, y se apoyan en la hipótesis de que soft-words expandidas deberían encontrarse ubicadas cerca tanto en la documentación como en textos generales.

La función de scoring se compone de tres categorías de métricas.
\paragraph{Características de las softwords.}
Corresponde a las carasterísticas propias de cada cadena de texto generada en la división.
Dentro de estas métricas encontramos:
\begin{itemize}
  \item Número de palabras.
  \item Tamaño promedio de la palabra.
  \item Palabra más larga.
  \item Palabras con vocales.
  \item Número de palabras con un solo caracter.
\end{itemize}

\paragraph{Información externa.}
Incluye diccionarios y otra información que es provista tanto por humanos como fuentes que no son código.
Se utilizan dos diccionarios, en donde uno, el \textit{pequeño} es un subconjunto del diccionario \textit{grande}.
Además, dos métricas hacen uso de la información de co-ocurrencia desarrollada por Google, en donde se consulta un conjunto de datos que indica el número de veces que una serie de cinco palabras se observa en las páginas indexadas por el motor de búsqueda hasta 2006.
Las métricas que caben dentro de esta categoría son:
\begin{itemize}
  \item Cantidad de matches en el diccionario pequeño.
  \item Cantidad de matches en el diccionario grande.
  \item Cantidad de matches en el diccionario grande con longitud 3.
  \item Cantidad de palabras de programación.
  \item Cantidad de expansiones del diccionario.
  \item Co-ocurrencia.
  \item Co-ocurrencia con longitud 3.
\end{itemize}

\paragraph{Información interna.}
Ésta se deriva del código fuente, así sea tanto del programa bajo análisis como de otros conjuntos de programas.
Además, se implementan también las tablas de frecuencia que son utilizadas en el Algoritmo Samurai, con la diferencia que dichas frecuencias se encuentran normalizadas.
En esta categoría las métricas son:
\begin{itemize}
  \item Presente en el código (ya que los caracteres forman un acrónimo para una frase encontrada en el código).
  \item Union-program probability (VER)
  \item Union-global probability (VER)
  \item Intersection-program probability (VER)
\end{itemize}

Como la variable de respuesta es binaria, es decir, si una división es correcta o incorrecta, se utilizó regresión logística para modelar la asociación entre la variable de respuesta y las variables explicativas (VER).
El modelo resultante es una combinación ponderada de las métricas estadísticamente significativas, y es la función de scoring que se aplica para evaluar divisiones potenciales.

El algoritmo emplea una función \textit{Splits(id)}, la cual determina el conjunto de todas las posibles divisiones de un identificador \textit{id}.
Una división $s \in Splits(id)$ está compuesta por una serie de soft-words $s_1 s_2 \dots s_n$.
Para cada soft-word $s_i$, la función $E(s_i)$ establece el conjunto de las distintas expansiones posibles $e_{i,1}, e_{i,2}, \dots, e_{i,m}$ para $s_i$.
En el algoritmo, la similitud entre dos expansiones, $sim(e_1, e_2)$ viene dada por la probabilidad de que $e_1$ y $e_2$ co-ocurran en una ventana de cinco palabras en el conjunto de datos de Google.

Para una división $s = s_1 s_2 \dots s_n$, por cada expansión $e_{i,j} \in E(s_i)$, se define el puntaje de la similiridad entre $e_{i,j}$ y las otras soft-words de $s$, donde $s_k (k \neq i)$ como la suma de las similaridades entre $e_{i,j}$ y los elementos de $E(s_k)$:

\begin{center}
  $\underset{(k \neq i)}{sim(e_{i,j}, s_k)} = \displaystyle\sum_{e \in E(s_k)} sim(e_{i,j}, e)$
\end{center}

La \textit{cohesión} para $e_{i,j}$ relativa a $s$ se define como:

\begin{center}
  $cohesion(e_{i,j}, s) = log \displaystyle [\sum_{s_k \in s \neq s_i} sim(e_{i,j}, s_k)]$
\end{center}

Por cada $s_i \in s$, $score(s_i)$ es la cohesión de la expansión $e_{i,j} \in E(s_i)$ que tiene el valor máximo de \textit{cohesion}:

\begin{center}
  $score(s_i) = max_{e_{i,j} \in E(s_i)} [cohesion(e_{i,j}, s)]$
\end{center}

En comparación con Greedy y Samurai, el algoritmo GenTest performa mucho mejor, con un nivel de precisón mayor.
En los casos en los que no logra obtener el resultado esperado, se observa que este algoritmo tiende a realizar menos divisiones que las esperadas (\textit{undersplit}).

\section{Expansión}
Una vez que el identificador se descompone, se prosigue a darle significado a cada una de las partes constituyentes.
Pueden ocurrir dos situaciones.
La primera es el caso fácil en donde la soft-word es una palabra de diccionario y tiene un significado bien establecido que condice con el uso que tiene dentro del código fuente.
En la segunda, para aquellas palabras que no estén en el diccionario, se asume que son abreviaturas o acrónimos, y es aquí donde se aplican los \textbf{algoritmos de expansión}.

El objetivo de un algoritmo de expansión es tomar una abreviatura o acrónimo como entrada, identificar a qué tipo pertenece, y luego por medio de diferentes técnicas obtener el significado asociado, siendo éste la salida del proceso.

\subsection{Algoritmo de Expansión Básico}
Este algoritmo, descrito por Lawrie et al. \cite{Lawrie:2007:EMA:1306878.1307350}, permite expandir abreviaturas y acrónimos en palabras y frases con significado, basándose en la información disponible a nivel local (de función).
Para llevar a cabo su propósito, utiliza cuatro listas de expansiones potenciales:
\begin{enumerate}[(a)]
  \item Una lista de \textbf{palabras del lenguaje natural} extraída desde el código fuente.
  \item Una lista de \textbf{frases} extraídas también desde el código fuente.
  \item Una lista de corte (\textit{stoplist}), que consiste en \textbf{palabras reservadas} del lenguaje de programación.
  \item Un \textbf{diccionario} del lenguaje natural.
\end{enumerate}

Durante la conformación de la lista de palabras del lenguaje natural extraídas desde el código (a) se emplean dos fuentes.
La primera consiste en los comentarios que aparecen antes o dentro de la función.
La segunda implica las hard-words de diccionario encontradas en los identificadores presentes en la función.

La lista de frases (b) se obtiene al pasar los comentarios y los identificadores multi-palabras a través de un buscador de frases \cite{Feng01}.
Aquí, la primera letra de cada palabra en la frase se utiliza para construir un acrónimo que luego se utiliza para las comparaciones.

En el proceso de extracción se utilizan dos técnicas de IR (\textit{Information Retrieval}), para mejorar la calidad del mismo.
\textit{Stemming} es la primera, y consiste en eliminar los sufijos de las palabras; por lo tanto, ignorando la forma particular de esa palabra remitiéndose a la raíz.
Por ejemplo: $stem(walk, walking, walk) \rightarrow walk$.
Se utiliza para determinar si las hard-words son palabras de diccionario, ya que el tipo particular de \textit{stemming} (Krovetz) recorta las palabras a las disponibles en diccionario.
La segunda técnica, \textit{stopping}, filtra soft-words que no son relevantes a través de una lista de corte.
Cuando se tiene en cuenta el código fuente, se usa una lista especial que tiene entradas específicas del lenguaje de programación.

Una vez que la lista de palabras y frases potenciales fue extraída, el proceso comienza. 
Este proceso básico de expansión, como puede verse en el algoritmo \ref{algLBF}, consta de dos pasos.
En el primero, se busca una expansión entre la lista de palabras y frases extraídas del código fuente.
Si no hay coincidencias, se continúa con el segundo paso, en donde la búsqueda se realiza sobre el diccionario.
Las palabras extraídas del código fuente, así como las que forman parte de la lista de corte, siempre tienen mayor prioridad que las del lenguaje natural.

Una abreviatura o acrónimo coincide con alguna palabra de las listas siempre y cuando comience con la misma letra, y el resto de sus letras individuales aparezcan, en orden, dentro de la palabra candidata.
Si hay una coincidencia en cualquier instancia de la búsqueda, se retorna como la expansión de la abrevitura/acrónimo.
El algoritmo sólo retorna un valor cuando existe una única coincidencia.
En caso de haber múltiples coincidencias, no se puede determinar cuál es la correcta, por lo que no se retorna ninguna.

\begin{algorithm}[H]
\caption{Expansión LBF}
\label{algLBF}
\DontPrintSemicolon
  \SetKwData{Abbrev}{abbrev}
  \SetKwData{WordList}{wordList}
  \SetKwData{PhraseList}{phraseList}
  \SetKwData{StopList}{stopList}
  \SetKwData{Dict}{dictionary}
  \SetKwData{Candidates}{candidates}
  \SetKwData{Expansions}{expansions}
  \SetKwData{Expansion}{expansion}
  
  \KwIn{\Abbrev, la abreviatura o acrónimo a expandir}
  \KwOut{\Expansion, la expansión resultante}
  
  \BlankLine
  var \WordList\;
  var \PhraseList\;
  var \StopList\;
  var \Dict\;
  
  \BlankLine
  \If{$\Abbrev \in \StopList$}{
    \KwRet \Abbrev\;
  }
  
  \BlankLine
  \If{$\Abbrev \in \PhraseList$}{
    \KwRet $\PhraseList.get(\Abbrev)$ \;
  }
  
  \BlankLine
  \If{$\Abbrev \in \WordList$}{
    \KwRet $\WordList.get(\Abbrev)$ \;
  }
  
  \BlankLine 
  $\Expansions \gets []$\;
  $\Candidates \gets \Dict.find(\Abbrev)$\;
  $\Expansions.add(\Candidates)$\;
  
  \BlankLine
  $\Expansion \gets null$\;
  \If{$\Expansions.size == 1$}{
    $\Expansion \gets \Expansions[0]$\;
  }
  
  \BlankLine
  \KwRet \Expansion\;

\end{algorithm}

Este algoritmo, suele acoplarse a la división de identificadores realizada por Greedy, y tiene la ventaja de que trabaja independientemente en el contexto del código fuente para una función particular, sin depender de otras partes del sistema. Sin embargo, esto hace que la visión sea acotada.

\subsection{Algoritmo AMAP}
AMAP, cuyas siglas representan \textit{Automatically Mining Abbreviations expansions in Programs}, es un algortimo que permite buscar expansiones potenciales y además seleccionar la que mejor se ajuste, en caso de que haya más de un resultado posible.
Se basa en la hipótesis de que el minado automático de las abreviaturas desde el programa mismo puede identificar las expansiones más apropiadas dentro del contexto de cada ocurrencia en particular.
Por lo tanto, no necesita disponer de un diccionario de palabras en lenguaje natural.

El algoritmo trabaja sobre varios tipos diferentes de palabras que no se encuentran en el diccionario.
Dentro de estos tipos tenemos las abreviaturas de una sola palabra (\textit{single word}), las abreviaturas de múltiples palabras {\textit{multi-word abbreviations}) y otros tipos de formas cortas.

Las abreviaturas de una sola palabra son aquellas formas cortas en las que sus expansiones, o formas largas, se componen de una sola palabra.
Dentro de esta categoría tenemos los \textbf{prefijos}, que se forman al descartar la última parte de una forma larga, reteniendo solamente unas pocas letras del principio. Un subconjunto de éstos son aquellos prefijos conformados por una sola letra.
También caen en esta categoría las \textbf{dropped letters}, las cuales son abreviaturas en las que se remueven una o más letras dentro de la forma larga, excepto la primera.

Las abreviaturas de múltiples palabras son aquellas en las que, como su nombre lo indica, cuando se expande la forma corta, el resultado es una forma larga compuesta por un grupo de palabras.
Pueden ser de dos tipos.
Por un lado estás los \textbf{acrónimos}, que se conforman con la primera letra de cada palabra de la expansión, y son la forma más común.
Por otro, la \textbf{combinación multi-palabra}, que incluye más de una letra por cada una de las palabras, y pueden ser abreviaturas de una sola palabra, acrónimos o palabras del diccionario.

Por último, los \textit{otros tipos de formas cortas} son aquellos en los que no están claros los límites de las múltiples palabras que las componen.
Aquí se incluyen tanto las palabras que a menudo aparecen adjacentemente una a la otra y representan un forma convencional de decir cosas, denominadas \textbf{collocations}; así como la \textbf{notación matemática y otros}.

La técnica de minado automático está inspirada en la representación del alcance de las variables en la tabla de símbolos para un compilador.
Para la búsqueda de formas largas potenciales, se comienza con el alcance más cercano a la abreviatura, como los nombres de tipos y sentencias, y gradualmente se extiende el alcance para incluir el método, sus comentarios y los comentarios de la clase.
Si la técnica continúa sin dar resultados, se expande la búsqueda al programa completo y al código de Java SE 1.5.
Con cada extensión del alcance se incluye información más general, y por lo tanto menos específica la búsqueda.

En el caso de las \textbf{palabras únicas}, el primer paso en la búsqueda de las formas largas consiste en construir una expresión regular, a partir de la forma corta, y luego utilizar ese patrón para analizar las diferentes partes del cuerpo del método, en pos de encontrar palabras candidatas.

\paragraph{Patrón prefijo:}
Se genera al agregarle la expresión regular \verb;"[a-z]+"; a la forma corta \textit{fc}, quedando \verb;"fc[a-z]+";.
La \verb;x; es un caso especial, ya que suele referirse a palabras que comienzan con \textit{ex}, por lo que al inicio se añade el fragmento \verb;"e?";.

\paragraph{Patrón dropped letters:}
Es mucho menos conservativo que el patrón usado para los prefijos.
Se construye insertando la expresión \verb;"[a-z]*"; después de cada letra de la forma corta.
Si la forma corta viene dada por $fc = c_0, c_1,\dots, c_n$, donde \textit{n} es la longitud, entonces el patrón resultante es $c_0$\verb;[a-z]*;$c_1$\verb;[a-z]*;$\dots$\verb;[a-z]*;$c_n$.HORRIBLE

Para las formas cortas de \textbf{múltiples palabras}, y debido a que los patrones deben buscar sobre espacios, es importante limitar qué tan lejos se puede extender la expresión.
Para ello, el texto que compone el cuerpo de un método y sus comentarios son preprocesados... VER
Los comentarios y las cadenas de texto son divididos en frases, a partir de los signos de puntuación (? ! , ; .).
Las palabras de corte, las cuales son comunes, se remueven.

\paragraph{Patrón acrónimo:}
Se conforma insertando la expresión \verb;"[a-z]+[ ]+"; después de cada letra en la forma corta.
Si ésta viene dada por $fc = c_0, c_1,\dots, c_n$, donde \textit{n} es la longitud, entonces el patrón que se genera es $c_0$\verb;[a-z]+[ ]+;$c_1$\verb;[a-z]+[ ]+;$\dots$\verb;[a-z]+[ ]+;$c_n$.HORRIBLE
Al igual que con los prefijos, la letra \verb;x; es un caso especial.
Cuando se compone un patrón acrónimo, cualquier ocurrencia de \verb;x; en la forma corta se reemplaza por la expresión \verb;"e?x";.
Esto le permite a la técnica encontrar expansiones para acrónimos como \textit{xml (eXtensible Markup Language)}.

\paragraph{Patrón combinación de palabras:}
Este patrón se obtiene añadiendo la expresión \verb;"[a-z]*?[ ]*?"; a cada una de las letras que pertenecen a la forma corta.
Sea $fc = c_0, c_1, \dots, c_n$, donde \textit{n} es la longitud, entonces el patrón resultante es $c_0$\verb;[a-z]*?[ ]*?;$c_1$\verb;[a-z]*?[ ]*?;$\dots$\verb;[a-z]*?[ ]*?;$c_n$.HORRIBLE
La expresión se construye de manera que sólo las letras que aparezcan en la forma corta puedan ser el inicio de una palabra.
Se utiliza una expresión más conservadora para favorecer las expansiones más cortas, con menos espacios.
Como la expresión ya es más general, pueden coincidir con expansiones incorrectas, por lo que la búsqueda se aplica solamente para formas cortas con una longitud de cuatro o más caracteres.

De acuerdo a los creadores del algoritmo, el mejor orden para aplicar los patrones viene dado por (1) acrónimos, (2) prefijo, (3) dropped letters y (4) combinación de palabras.

A medida que se expande el espectro de búsqueda, como métodos o comentarios, es posible que un tipo de patrón coincida con varias expansiones potenciales.
Para resolver estos casos de \textbf{múltiples coincidencias}, se sigue un determinado número de pasos, hasta encontrar la expansión que mejor aplique a la situación.
Estos pasos son:
\begin{enumerate}
  \item Utilizar la expansión que coincida más frecuentemente con la forma corta, dentro del alcance.
  Por ejemplo, si "value" coincidió el patrón prefijo para "val" tres veces y "valid" sólo una, entonces se elige "value".
  \item Agrupar las palabras con la misma raíz y actualizar las frecuencias en base a esto.
  Por ejemplo, si para el patrón prefijo sobre 'def' coinciden las palabras 'default' (2 veces), 'defaults' (2 veces), y 'define' (2 veces), se agrupan 'default' y 'defaults' en la forma más corta, siendo 'default' (4 veces), y por lo tanto se retorna esta, ya que tiene la mayor frecuencia.
  \item Si todavía no hay un ganador claro, continuar buscando por el patrón en alcances mayores.
  Se deben almacenar las frecuencias de las expansiones encontradas, así las más frecuentes son las más favorecias a medida que extendemos el alcance.
  \item Si todo lo anterior falla, abandonar la búsqueda y dejar que EMF elija la expansión.
\end{enumerate}

\subsubsection{Expansión Más Frecuente (EMF)}


\subsection{Algoritmo Normalize}
