\chapter{Análisis de Identificadores}

\section{Identificadores}
Formalmente, un identificador t es una secuencia de caracteres c0, c1, …, cn, dondel el caracter ci representa una letra, un dígito o un caracter especial. Hard words, soft words. COMPLETAR

Ver Rilling y Klemola - Identifying comprehension bottlenecks using program slicing and cognitive complexity metrics.

Ver Caprille y Tonella - Reestructuring program identifier names.

\subsection{Identificadores concisos y consistentes}
Cada lenguaje de programación plantea sus propios estilos de codificación, así como también convenciones para los nombres de los identificadores. Sin embargo, éstas suelen enfocarse solamente en la sintaxis, dejando de lado uno de los aspectos de mayor interés, el cual se corresponde a la semántica de los nombres empleados. La importancia en la definición de nombres adecuados radica en la influencia que los mismos tienen durante la comprensión de programas, y como consecuencia, en la calidad y productividad a lo largo del ciclo de vida del software. La ofuscación de código a través de la conversión de identificadores a secuencias aleatorias de caracteres es un claro ejemplo del impacto que tienen los nombres en el proceso de comprensión.

DeiBenbock y Pizka definen, a través de un \textbf{modelo formal}, una estrategia para la creación de nombres bien formados. En ésta se incluyen dos reglas, las cuales permiten un nombramiento conciso y consistente de variables, funciones y clases \cite{DeiBenbockPizka05}. La premisa sobre la que basan estas reglas viene dada por la correlación de identificadores con el conjunto de conceptos que son utilizados en un programa.

\subsubsection{Conceptos nombrados}
Primero, se establece a \textit{C} como el conjunto de todos los conceptos relevantes dentro de un determinado alcance (un componente de un programa, el dominio de la aplicación o una organización). A su vez, se define un concepto como una unidad con un significado asociado en términos de comportamiento o propiedades. Se modelan también todos los nombres posibles y se denotan por \textit{N}, junto con la asignación de nombres a conceptos como una relación formal $R \subseteq N \times C$.

\subsubsection{Regla 1: Consistencia}
Del lenguaje natural heredamos dos tipos de incosistencias: homónimos y sinónimos.

Un \textbf{homónimo} es aquella palabra que se escribe o pronuncia igual que otra, pero tiene diferente significado, y su definición formal viene dada por: un nombre $n \in N$ es llamado homónimo sí y solo sí $|C_n| > 1$ donde $C_n = \{ c \in C : (n,c) \in R\}$.

A su vez, un \textbf{sinónimo} es aquella palabra que, si bien se escribe de forma diferente, refiere al mismo concepto que otra. La definición formal se establece como: dos nombres $s,n \in N$ son sinónimos sí y sólo sí $C_s \cap C_n \neq 0$.

Ante la ausencia de homónimos, el daño de los sinónimos es limitado ya que siempre apuntan a un único concepto. Sin embargo, incrementan innecesariamente el dominio \textit{N} y la relación \textit{R}, y por consiguiente aumentan el esfuerzo para comprender el lenguaje utilizado. La presencia de homónimos y sinónimos tiene un impacto negativo importante ya que para cada identificador \textit{n}, el desarrollador tiene que considerar todos los conceptos en
\\
\\$M_n = \bigcup_{e \in n} C_e$
\\
\\donde $S_n$ es el conjunto de todos los nombres sinónimos a \textit{n} (incluyendo a \textit{n} mismo).

Dadas estas condiciones, podemos decir que un sistema de nombres \textit{C}, \textit{N} y \textit{R} es \textit{consistente} sí y sólo sí $R \subseteq N \times C$ es una relación bijectiva. Por lo tanto, se define como:
\\$n : C \rightarrow N$
\\$n(c) = \mbox{nombre único del concepto} \ c$.

\subsubsection{Regla 2: Conciso}
Para definir \textit{conciseness} debemos introducir el orden parcial $\sqsubset$ para el conjunto de conceptos \textit{C}, de acuerdo a su nivel de abstracción.

Dado el conjunto \textit{P}, el cual contiene elementos del programa que son identificados como unidades a través de un nombre simbólico, y dada \textit{i}, que representa el mapeo de los elementos del programa con sus identificadores.
\\$i : P \rightarrow N$
\\$i(p) = \mbox{identificador de} \ p$.

Además, dado $\lbrack c \rbrack$, el cual denota la semántica en el sentido del significado de un concepto $c \in C$. De la misma manera, $\lbrack p \rbrack$ denota la semántica de un elemento del programa \textit{p}.

Establecidas estas condiciones, definimos el problema de \textit{conciseness} en dos pasos. Primero se requiere la \textit{correcta} identificación, y luego la validación de \textit{conciseness}.

\paragraph{Definición: correctitud}
Sea $p \in P$ un elemento del programa y $c \in C$ el concepto que implementa, tal que $\lbrack p \rbrack = \lbrack c \rbrack$. El identificador \textit{i(p)} es \textit{correcto} sí y solo sí ocurre lo siguiente:
\\$i(p) \in \lbrace n(c') : c' \in C \land c' \sqsupseteq c \rbrace$
\\Esto significa que el identificador de un elemento del programa \textit{p} que manifiesta el concepto \textit{c} debe corresponderse al nombre de \textit{c} o a una generalización del mismo. Sin embargo, a veces ocurre que de alguna manera, los nombres de los identificadores son correctos pero no lo suficientemente consisos. Para contrarestar esta situación, se agrega el siguiente requerimiento:

\paragraph{Definición: conciso}
Sea $p \in P$ un elemento del programa y $c \in C$ el concepto que implementa, tal que $\lbrack p \rbrack = \lbrack c \rbrack$. El identificador \textit{i(p)} para el elemento \textit{p} del programa es \textit{conciso} sí y sólo sí lo siguiente es verdad:
\\$i(p) = n(c)$
\\Esta definición requiere que un identificador tenga exactamente el mismo nombre del concepto que representa.

INSERTAR IMAGENES

\subsubsection{Consistencia y Conciseness sintácticas}
El modelo formal descrito por DeiBenbock y Pizka requiere que exista un mapeo entre los elementos del dominio de conceptos y los identificadores. Sin esta definición, se hace imposible la relación entre los dos conjuntos. Ahora bien, para nuevos programas, la construcción de este mapeo puede hacerse al mismo tiempo que el desarrollo con un mínimo costo extra. Sin embargo, para aquellos proyectos existentes, el costo puede ser demasiado. Para contrarestar esta limitación, se plantea el uso de las \textit{consistencia y conciseness sintáticas}, en donde sólo se considera la construcción sintática de los identificadores \cite{LawrieFeildBinkley06}.

El enfoque se basa en la contención de identificadores. Se dice que un identificador está contenido dentro de otro si todas sus \textit{soft words} están presentes, en el mismo orden, en el identificador contenedor. Cuando un identificador está contenido dentro de otro, una de dos posibles violaciones han ocurrido. Por un lado, puede que haya un solo concepto asociado a dos identificadores, lo que implica una violación al requerimiento respecto a los sinónimos en la consistencia; por otro, que los dos identificadores refieran a diferentes conceptos. En este caso, no se cumpliría la regla de nombres concisos.

INSERTAR IMAGENES

\paragraph{Definición: Conciseness y Consistencia de sinónimos sintática}
Sea el identificador $id_1$ una secuencia de soft words $sw_1 sw_2 ... sw_n1$. Los identificadores $id_1$ e $id_2$ no cumplen el \textit{requerimiento sobre sinónimos en la consistencia sintáctia} si $id_2$ incluye la secuencia de soft words $w_1 w_2 ... sw_1 sw_2 ... sw_n1 ... w_n2$. Además, $id_1$ falla el \textit{requerimiento sobre la concisenss sintáctica} si existe un tercer identificador $id_3$ que incluya la secuencia de soft words $u_1 u_2 ... sw_1 sw_2 ... sw_n1 ... u_n3$.

\section{División}
El primer paso para analizar los identificadores que se utilizan consiste en dividir cada uno de ellos, en los elementos que lo conforman. Los desarrolladores componen estos identificadores concatenando palabras y abreviaciones, a veces, empleando convenciones claras en la demarcación de sus partes, como lo son el uso de caracteres no alfabéticos (por ejemplo: ``\_'' o ``-''), o la técnica camel-case, donde la primera letra de cada palabra se escribe en mayúsculas a excepción de la inicial. Esta situación permite que la división automática se lleve a cabo sin ningún problema. Sin embargo, en los casos en los que no se implementa una clara delimitación es donde surge la necesidad de aplicar técnicas más sofisticadas para lograr la separación de las partes. Aquí es donde entran en juego los \textbf{algoritmos de división}.
 
El objetivo de un algoritmo divisor de identificadores es tomar uno de estos últimos como entrada, y obtener como salida una lista de sub-elementos que particionan al identificador original. Estos sub-elementos pueden ser palabras de diccionario, las cuales tienen un significado obvio; abreviaturas, las cuales refieren a una sola palabra del diccionario; o acrónimos, los cuales representan varias palabras de diccionario \cite{HillBinkleyLawrie14}.

\subsection{El problema de la división de tokens}
Ahora bien, estos algoritmos deben ser capaces de afrontar una serie de problemas relacionados a la correcta división de los identificadores. Como se dijo anteriormente, una forma de establecer una separación entre las palabras componentes de un indicador es el uso de caracteres especiales, o a través de la técnica de \textit{camel-casing}. Esta última permite reducidir la cantidad de caracteres sin comprometer la legibilidad, aunque existen situaciones donde no hay convenciones establecidas y esto puede afectar negativamente la comprensión, siendo ejemplos de este caso la incorporación de acrónimos al identificador (ej: \textit{sqlList, SQLList, SQLlist}) o la no utilización de delimitadores para conceptos multi palabras que son de uso común, como en \textit{sizeof} o \textit{hostname}.

Un token puede ser definido formalmente como $t = (s_0, s_1, s_2, ..., s_n)$ donde $s_i$ es una letra, un dígito o un caracter especial. El primer paso consiste en separar el token a través utilizando los dígitos y caracteres especiales como marcadores. Cada sub-string es considerado como un token potencialmente divisible, recibe el nombre de \textit{token alfabético}, y existen cuatro casos posibles a la hora de decidir si dividirlo en un cierto punto entre $s_i$ y $s_j$\cite{EnslenHillPollock09}:

\begin{enumerate}
  \item $s_i$ está en minúsculas y $s_j$ en mayúsculas (ej: getString, setPoint)
  \item $s_i$ está en mayúsculas y $s_j$ en minúsculas (ej: getMAXstring, GPSstate, ASTVisitor)
  \item tanto $s_i$ como $s_j$ están en minúsculas (ej: notype, databasefield, actionparameters)
  \item tanto $s_i$ como $s_j$ están en mayúsculas (ej: USERLIB, NONNEGATIVEDECIMALTYPE, COUNTRYCODE)
\end{enumerate}

El primer caso, es el lugar natural para realizar la división. En el segundo caso, en el cual el lugar donde dividir existen alternaciones de mayúsculas y minúsculas, presenta el problema de la separación de tokens \textit{mixed-case}, particularmente complicado por la utilización de acrónimos. Por último, los casos 3 y 4 corresponden al problema de la separación de tokens \textit{same-case}; en donde los programadores no han dejado rastro donde cualquier palabra o concepto deba ser extraída del token.

Un algoritmo de división de tokens completamente automático debería ser capaz de resolver los sub-problemas de mixed-case y same-case, de forma efectiva.

\subsection{Estado del arte}
De acuerdo a la recopilación de Hill et. all (Cómo se pone?), algunas de las principales técnicas para la división de identificadores son:

\begin{itemize}
  \item \textbf{Greedy.} Esta técnica utiliza un diccionario, una lista de abreviaturas conocidas y una lista de corte, la cual incluye identificadores predefinidos, liberías, funciones, nombres de variables comunes y letras individuales. Después de retornar cada hard word encontrada en alguna de las tres listas, como una soft word simple, el resto de las hard words se consideran para división. A partir de ahí, recursivamente, se analizan los sufijos y prefijos de las palabras hasta que se encuentren en alguna de las listas, prefiriendo siempre las palabras de mayor longitud.
  
  \item \textbf{Samurai.} Este algoritmo se basa en la premisa de que las partes que componen a los identifcadores multi-palabras de un determinado programa, suelen ser utilizados en algún otro lugar, así sea en el mismo software como en otros. Por lo tanto, la frecuencia de aparición de las palabras es el principal elemento considerado a la hora de dividir identificadores.
  
  \item \textbf{GenTest.} Tiene su fuerte en la división de términos same-case, generando primero todas las posibles divisiones (dado que los identificadores son relativamente cortos, el potencialmente exponencial número de fragmentos es manejable en la práctica), para luego aplicar un \textit{scoring} sobre cada elemento, seleccionando la de mayor valor. Esta función de scoring se apoya en que las soft words iguales o similares, deberían encontrarse ubicadas cerca una de otra en la documentación o texto general (métrica de similaridad). 
  
  \item \textbf{DTW.} Esta técnica se basa en la observación de que los programadores construyen nuevos identificadores aplicando un conjunto de transformaciones a las palabras, como por ejemplo quitar todas las vocales o algunos caracteres. Utilizando un diccionario que contenga palabras y terminos pertenecientes a una ontología superior, al dominio de la aplicación o ambos, el objetivo consiste en identificar un matcheo cuasi-óptimo entre las partes del identificador y las palabras en el diccionario.
  
  \item \textbf{INTT.} El enfoque INTT busca realizar una división más precisa que las técnicas previas al utilizar una heurística especializada para manejar identificadores con dígitos, sin aplicar la separación de los digitos del resto del texto en etapas tempranas del proceso de división. Las principales modificaciones consisten en reemplazar greedy por dos algoritmos, greedy y greedier, los cuales pueden reconocer las partes de los identificadores same-case, sin requerir que comiencen o terminen con una palabra conocida; y además utilizar una lista de acrónimos que contiene dígitos.
\end{itemize}

Para el presente informe, sólo son de interés - y por lo tanto explicadas en mayor detalle - las primeras tres técnicas listadas anteriormente.

\subsection{Algoritmo Greedy}
Desarrollado por Feild, Binkley and Lawrie \cite{FieldBinkleyLawrie06, Feild06anempirical, Lawrie2007, Lawrie:2007:EMA:1306878.1307350, EnslenHillPollock09}, este algoritmo recibe su nombre gracias al enfoque que toma para resolver el problema, eligiendo en cada paso local la opción más conveniente con la idea de lograr, a nivel general, una solución óptima. El proceso comienza realizando una búsqueda por cada hard word dentro de un conjunto de listas de palabras y abreviaturas. Si la palabra candidata existe en alguna de ellas, se devuelve como válida, si no, se asume que se compone de múltiples palabras, y éstas son tratadas recursivamente hasta encontrar sus componentes \cite{Feild06anempirical}. Las tres listas que se utilizan son:

\begin{itemize}
  \item \textbf{Diccionario:} se conforma con diccionarios de dominio público, los cuales son consultados en la herramienta de verificación de gramática \textit{ispell}, disponible en Linux.
  
  \item \textbf{Abreviaturas conocidas:} consiste de abreviaturas que son de uso común y relacionadas tanto al dominio del problema como a la programación.
  
  \item \textbf{Stop-list:} es la lista de corte y está compuesta de tres tipos de palabras, dentro de las cuales encontramos a los tipos de datos predefinidos, variables globales (tanto del lenguage como del ambiente), y los nombres de librerías estándar.
\end{itemize}

Tal como puede verse en el algorimo \ref{algGreedy}, el proceso comienza dividiendo el token de entrada en hardwords, tomando como delimitadores a los caracteres especiales y la técnica de \textit{camel-casing}.
Una vez hecho tanto el reemplazo de números y caracteres especiales (línea 4), como la división inicial del token (línea 5), se procede con la evaluación de cada una de las hardwords resultantes.
En primera instancia, se verifica si la palabra candidata existe en alguna de las listas con las que trabaja el algoritmo (línea 8). Si es así, el fragmento del token es considerado parte de la división y por lo tanto de la salida del proceso (línea 13).
En caso contrario, al asumirse que está formado por múltiples palabras, se procesa en búsqueda de las softwords que lo componen.
Este análisis consiste en realizar dos búsquedas sobre cada hardword, quedándose con el mejor resultado entre ambas.
En la primera búsqueda (línea 9) se trata de obtener el prefijo más largo que se encuentre en una de las listas, y forme parte de la palabra actual. Luego se llama recursivamente, trabajando sobre el fragmento restante de la palabra original (prefijos y sufijos).
La segunda búsqueda (línea 10) es idéntica, excepto que trata de obtener el sufijo más largo.
Los resultados de ambos procesamientos son evaluados, y aquel que arroje la relación más alta entre la cantidad de softwords encontradas en listas vs. el total de palabras, es el elegido (línea 11).
Al finalizar la ejecución del algoritmo, la salida que se obtiene es el identificador original dividido, separando cada una de los componentes encontrados, a través de espacios en blanco (línea 16).

\begin{algorithm}[H]
\caption{Greedy}
\label{algGreedy}
\DontPrintSemicolon
  \SetKwData{Token}{token}
  \SetKwData{SplitToken}{splitToken}
  \SetKwData{Dictionary}{dictionary}
  \SetKwData{KnownAbbrs}{knownAbbreviations}
  \SetKwData{StopList}{stopList}
  \SetKwData{Preffix}{preffix}
  \SetKwData{Suffix}{suffix}
  
  \SetKwFunction{SplitChars}{splitOnSpecialCharactersAndDigits(\Token)}
  \SetKwFunction{SplitLowerUpperCase}{splitOnLowerCaseToUpperCase(\Token)}
  \SetKwFunction{FindPreffix}{findPreffix($s_i$,$""$)}
  \SetKwFunction{FindSuffix}{findSuffix($s_i$,$""$)}
  \SetKwFunction{Compare}{compare(\Preffix,\Suffix)}
  
  \KwIn{\Token, para ser divido}
  \KwOut{\SplitToken, token dividido delimitado por espacios}
  
  \BlankLine
  var \Dictionary\;
  var \KnownAbbrs\;
  var \StopList\;
  
  \BlankLine
  \Token $\leftarrow$ \SplitChars\;
  \Token $\leftarrow$ \SplitLowerUpperCase\;
  \SplitToken $\leftarrow$ $""$\;
  
  \BlankLine
  \ForEach{$s_i \in \Token$}{
    \eIf{$s_i \not \in (\Dictionary \cup \KnownAbbrs \cup \StopList)$}{
      \Preffix $\leftarrow$ \FindPreffix\;
      \Suffix $\leftarrow$ \FindSuffix\;
      \SplitToken $\leftarrow$ \Compare\;
    }
    {\SplitToken $\leftarrow$ $s_i$}
  }
  \BlankLine
  \KwRet \SplitToken\;
\end{algorithm}

La función \textit{findPreffix} es descrita en el algoritmo \ref{funFindPreffix}. 
La primer validación que se realiza sirve como punto de corte para la recursión (línea 1), cuando el token a analizar está vacío.
Si el parámetro recibido es parte de alguna de las listas (línea 4), la función finaliza separando el prefijo encontrado y llamándose recursivamente con el fragmento restante (línea 5).
En caso contrario, el último caracter es removido (línea 8) y el proceso continúa llamándose a si mismo (línea 9).
Los caracteres removidos son agrupados (línea 7) y luego agregados al resultado. 
Si la primera palabra en el identificador dividido no está en una de las listas, los caracteres se van acoplando. Si lo está, se agregan como una nueva softword.

\begin{algorithm}
\caption{Función findPreffix}
\label{funFindPreffix}
\DontPrintSemicolon
  \SetKwData{S}{s}
  \SetKwData{SSplit}{sSplit}
  \SetKwData{Dictionary}{dictionary}
  \SetKwData{KnownAbbrs}{knownAbbreviations}
  \SetKwData{StopList}{stopList}
  
  \SetKwFunction{Size}{size}
  \SetKwFunction{FindPreffixSSplitEmpty}{findPreffix(\SSplit,$""$)}
  \SetKwFunction{FindPreffixSSSplit}{findPreffix(\S,\SSplit)}
  
  \KwIn{\S, string para ser analizado}
  \KwOut{\SSplit, string dividido y delimitado por espacios}
  
  \BlankLine
  \If{$\S.\Size == 0$}{
    \KwRet \SSplit
  }
  
  \BlankLine
  \If{$\S \in (\Dictionary \cup \KnownAbbrs \cup \StopList)$}{
    \KwRet $\S + "" + \FindPreffixSSplitEmpty$ 
  }
  
  \BlankLine
  \SSplit $\leftarrow \S[length(\S) - 1] + \SSplit$\;
  
  \BlankLine
  \S $\leftarrow \S[0, length(\S) - 1]$\;
  
  \BlankLine
  \KwRet \FindPreffixSSSplit
\end{algorithm}

Una lógica similar se sigue para la función \textit{findSuffix}, visible en el algoritmo \ref{funFindSuffix}. 
Cuando el parámetro se encuentra de alguna de las listas (línea 4), el sufijo se extrae y la función continúa llamándose recursivamente (línea 5).
En caso contrario, el primer caracter es removido (línea 8) y el proceso continúa llamándose a si mismo (línea 9), reduciendo la longitud del sufijo a evaluar.
Al igual que en la otra función, los caracteres removidos son agrupados (línea 7) y luego agregados al resultado.

\begin{algorithm}
\caption{Función findSuffix}
\label{funFindSuffix}
\DontPrintSemicolon
  \SetKwData{S}{s}
  \SetKwData{SSplit}{sSplit}
  \SetKwData{Dictionary}{dictionary}
  \SetKwData{KnownAbbrs}{knownAbbreviations}
  \SetKwData{StopList}{stopList}
  
  \SetKwFunction{Size}{size}
  \SetKwFunction{FindSuffixSSplitEmpty}{findSuffix(\SSplit,$""$)}
  \SetKwFunction{FindSuffixSSSplit}{findSuffix(\S,\SSplit)}
  
  \KwIn{\S, string para ser analizado}
  \KwOut{\SSplit, string dividido y delimitado por espacios}
  
  \BlankLine
  \If{$\S.\Size == 0$}{
    \KwRet \SSplit
  }
  
  \BlankLine
  \If{$\S \in (\Dictionary \cup \KnownAbbrs \cup \StopList)$}{
    \KwRet $\FindSuffixSSplitEmpty + "" + \S$ 
  }
  
  \BlankLine
  \SSplit $\leftarrow \SSplit + \S[0] $\;
  
  \BlankLine
  \S $\leftarrow \S[1, length(\S)]$\;
  
  \BlankLine
  \KwRet \FindSuffixSSSplit
\end{algorithm}

Este algoritmo, en comparación con otros existentes, tiene la ventaja de que es independiente del conjunto de datos sobre el que se ejecuta. Además, su lógica e implementación, son sencillas, lo que lo convierte en el punto de comparación a la hora de evaluar la performance de otras propuestas.

Sin embargo, tiene algunos aspectos negativos. El tiempo de ejecución puede ser elevado, ya que la inicialización está dominada por la carga de varios diccionarios en tablas \textit{hash} y por la utilización de recursividad \cite{FieldBinkleyLawrie06}.
Otra desventaja que presenta es que tiende a realizar una mayor cantidad de divisiones que las esperadas \cite{Feild06anempirical}.

\subsection{Algoritmo Samurai}
Propuesto por Hill et al., este algoritmo está inspirado en un trabajo previo, sobre minado automático para la expansión de abreviaturas \cite{Hill:2008:AAM:1370750.1370771}.
Las hipótesis sobre las que basan la lógica del proceso son que, en primera instancia, los strings que componen los tokens multi-palabras tienden a ser empleados en alguna otra parte del código, tanto del mismo programa como de otros. Este uso puede ser tanto como una palabra independiente, o como parte de una composición.
En segunda instancia, se apoyan en la hipótesis de que las divisiones suelen ser en favor de aquellos tokens que ocurren más a menudo en un programa. Por lo tanto, la frecuencia de una palabra se utiliza para determinar los cortes en el token analizado.

Para poder llevar adelante la división, es necesario explorar el conjunto de tokens existentes en el código y crear dos tablas de frecuencia.
Este proceso de minado se realiza ejecutando el algoritmo de división de tokens conservativo basado en marcadores de separación sobre un conjunto de tokens del código fuente, para así generar un listado de todas las ocurrencias de los strings delimitados por marcadores.
Este listado es luego procesado para obtener la información de frecuencias, en la forma de una tabla de búsqueda que almacena el número de ocurrencias para una determinada y única palabra.
Al trabajar sobre los tokens extraídos del código fuente bajo análisis, se construye  la \textit{tabla de frecuencias específica del programa}.
La segunda tabla, construída a partir de un conjunto mayor de programas, recibe el nombre de \textit{tabla de frecuencias global}.
Ambas tablas son empleadas en la función de scoring que se utiliza para evaluar las posibles divisiones durante el proceso de separación.



\subsection{Algoritmo GenTest}

\section{Expansión}
\subsection{Algoritmo de Expansión Básico}
\subsection{Algoritmo AMAP}
\subsection{Algoritmo Normalize}
